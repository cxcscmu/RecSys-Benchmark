W0307 21:07:31.244536 22518009701376 torch/distributed/run.py:779] 
W0307 21:07:31.244536 22518009701376 torch/distributed/run.py:779] *****************************************
W0307 21:07:31.244536 22518009701376 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0307 21:07:31.244536 22518009701376 torch/distributed/run.py:779] *****************************************
03/07/2025 21:07:38 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/07/2025 21:07:38 - INFO - __main__ -   Training/evaluation parameters DRTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
distil_mode=pairwise,
distillation=False,
do_encode=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=10,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gc_p_chunk_size=32,
gc_q_chunk_size=4,
grad_cache=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0007,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/runs/Mar07_21-07-37_babel-13-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
negatives_x_device=False,
no_cuda=False,
num_train_epochs=30.0,
optim=adamw_hf,
output_dir=/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=48,
per_device_train_batch_size=48,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=TASTE_amzn-beauty,
save_on_each_node=False,
save_steps=10,
save_strategy=steps,
save_total_limit=2,
seed=2022,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mapping_dataset=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
03/07/2025 21:07:38 - INFO - __main__ -   MODEL parameters ModelArguments(model_name_or_path='/data/group_data/cx_group/self_rewarding_framework/jingyuah/self_reward_rs/pretrained_model/t5_base', target_model_path=None, config_name=None, tokenizer_name=None, processor_name=None, cache_dir=None, untie_encoder=False, feature='last_hidden_state', pooling='first', add_linear_head=False, projection_in_dim=768, projection_out_dim=768, dtype='float32', encoder_only=False, pos_token=None, neg_token=None, normalize=False, param_efficient_method=None)
03/07/2025 21:07:38 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, 16-bits training: False
03/07/2025 21:07:38 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
03/07/2025 21:07:38 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
03/07/2025 21:07:38 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
03/07/2025 21:07:39 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
03/07/2025 21:07:39 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
03/07/2025 21:07:39 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1776
  Num Epochs = 30
  Instantaneous batch size per device = 48
  Total train batch size (w. parallel, distributed & accumulation) = 384
  Gradient Accumulation steps = 1
  Total optimization steps = 150
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: jingyuanhe1222. Use `wandb login --relogin` to force relogin
[rank4]:[W307 21:08:37.412566132 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W307 21:08:38.716544834 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W307 21:08:38.891771211 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W307 21:08:38.443570040 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W307 21:08:38.571802940 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W307 21:08:39.713592842 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W307 21:08:39.821101912 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/jingyuah/REC_source/TASTE/reproduce/train/beauty/wandb/run-20250307_210832-2z0d5h9c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TASTE_amzn-beauty
wandb: ⭐️ View project at https://wandb.ai/jingyuanhe1222/RecSys-Benchmark
wandb: 🚀 View run at https://wandb.ai/jingyuanhe1222/RecSys-Benchmark/runs/2z0d5h9c
  0%|          | 0/150 [00:00<?, ?it/s][rank0]:[W307 21:08:49.352554162 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  1%|          | 1/150 [00:28<1:09:40, 28.06s/it]                                                   1%|          | 1/150 [00:28<1:09:40, 28.06s/it]  1%|▏         | 2/150 [00:43<50:50, 20.61s/it]                                                   1%|▏         | 2/150 [00:43<50:50, 20.61s/it]  2%|▏         | 3/150 [01:03<50:02, 20.42s/it]                                                 2%|▏         | 3/150 [01:03<50:02, 20.42s/it]  3%|▎         | 4/150 [01:27<52:53, 21.73s/it]                                                 3%|▎         | 4/150 [01:27<52:53, 21.73s/it]  3%|▎         | 5/150 [01:45<49:09, 20.34s/it]                                                 3%|▎         | 5/150 [01:45<49:09, 20.34s/it]  4%|▍         | 6/150 [02:12<54:22, 22.66s/it]                                                 4%|▍         | 6/150 [02:12<54:22, 22.66s/it]  5%|▍         | 7/150 [02:33<53:00, 22.24s/it]                                                 5%|▍         | 7/150 [02:33<53:00, 22.24s/it]  5%|▌         | 8/150 [02:51<49:13, 20.80s/it]                                                 5%|▌         | 8/150 [02:51<49:13, 20.80s/it]  6%|▌         | 9/150 [03:12<48:44, 20.74s/it]                                                 6%|▌         | 9/150 [03:12<48:44, 20.74s/it]  7%|▋         | 10/150 [03:29<46:01, 19.73s/it]                                                  7%|▋         | 10/150 [03:29<46:01, 19.73s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A  7%|▋         | 10/150 [03:35<46:01, 19.73s/it]
100%|██████████| 1/1 [00:00<00:00,  1.30it/s][A
                                             [A03/07/2025 21:12:15 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-10
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-10/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-10/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-10/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-10/special_tokens_map.json
  7%|▋         | 11/150 [05:03<1:38:31, 42.53s/it]                                                    7%|▋         | 11/150 [05:04<1:38:31, 42.53s/it]  8%|▊         | 12/150 [05:27<1:24:39, 36.81s/it]                                                    8%|▊         | 12/150 [05:27<1:24:39, 36.81s/it]  9%|▊         | 13/150 [05:46<1:11:21, 31.25s/it]                                                    9%|▊         | 13/150 [05:46<1:11:21, 31.25s/it]  9%|▉         | 14/150 [06:05<1:02:41, 27.66s/it]                                                    9%|▉         | 14/150 [06:05<1:02:41, 27.66s/it] 10%|█         | 15/150 [06:24<56:29, 25.11s/it]                                                   10%|█         | 15/150 [06:24<56:29, 25.11s/it] 11%|█         | 16/150 [06:48<54:58, 24.62s/it]                                                 11%|█         | 16/150 [06:48<54:58, 24.62s/it] 11%|█▏        | 17/150 [07:07<50:54, 22.97s/it]                                                 11%|█▏        | 17/150 [07:07<50:54, 22.97s/it] 12%|█▏        | 18/150 [07:31<51:26, 23.38s/it]                                                 12%|█▏        | 18/150 [07:31<51:26, 23.38s/it] 13%|█▎        | 19/150 [07:54<50:33, 23.15s/it]                                                 13%|█▎        | 19/150 [07:54<50:33, 23.15s/it] 13%|█▎        | 20/150 [08:10<45:57, 21.21s/it]                                                 13%|█▎        | 20/150 [08:11<45:57, 21.21s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 13%|█▎        | 20/150 [08:17<45:57, 21.21s/it]
100%|██████████| 1/1 [00:00<00:00,  1.48it/s][A
                                             [A03/07/2025 21:16:57 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-20
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-20/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-20/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-20/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-20/special_tokens_map.json
 14%|█▍        | 21/150 [09:56<1:39:58, 46.50s/it]                                                   14%|█▍        | 21/150 [09:56<1:39:58, 46.50s/it] 15%|█▍        | 22/150 [10:17<1:23:08, 38.98s/it]                                                   15%|█▍        | 22/150 [10:18<1:23:08, 38.98s/it] 15%|█▌        | 23/150 [10:41<1:12:47, 34.39s/it]                                                   15%|█▌        | 23/150 [10:42<1:12:47, 34.39s/it] 16%|█▌        | 24/150 [11:07<1:06:52, 31.85s/it]                                                   16%|█▌        | 24/150 [11:07<1:06:52, 31.85s/it] 17%|█▋        | 25/150 [11:28<59:29, 28.56s/it]                                                   17%|█▋        | 25/150 [11:28<59:29, 28.56s/it] 17%|█▋        | 26/150 [11:51<55:55, 27.06s/it]                                                 17%|█▋        | 26/150 [11:51<55:55, 27.06s/it] 18%|█▊        | 27/150 [12:13<52:09, 25.45s/it]                                                 18%|█▊        | 27/150 [12:13<52:09, 25.45s/it] 19%|█▊        | 28/150 [12:32<47:45, 23.49s/it]                                                 19%|█▊        | 28/150 [12:32<47:45, 23.49s/it] 19%|█▉        | 29/150 [12:53<45:44, 22.68s/it]                                                 19%|█▉        | 29/150 [12:53<45:44, 22.68s/it] 20%|██        | 30/150 [13:16<45:48, 22.90s/it]                                                 20%|██        | 30/150 [13:16<45:48, 22.90s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 20%|██        | 30/150 [13:22<45:48, 22.90s/it]
100%|██████████| 1/1 [00:00<00:00,  1.17it/s][A
                                             [A03/07/2025 21:22:02 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-30
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-30/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-30/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-30/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-30/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-10] due to args.save_total_limit
 21%|██        | 31/150 [15:01<1:34:04, 47.43s/it]                                                   21%|██        | 31/150 [15:01<1:34:04, 47.43s/it] 21%|██▏       | 32/150 [15:25<1:19:52, 40.62s/it]                                                   21%|██▏       | 32/150 [15:26<1:19:52, 40.62s/it] 22%|██▏       | 33/150 [15:46<1:07:37, 34.68s/it]                                                   22%|██▏       | 33/150 [15:46<1:07:37, 34.68s/it] 23%|██▎       | 34/150 [16:12<1:01:50, 31.99s/it]                                                   23%|██▎       | 34/150 [16:12<1:01:50, 31.99s/it] 23%|██▎       | 35/150 [16:37<57:03, 29.77s/it]                                                   23%|██▎       | 35/150 [16:37<57:03, 29.77s/it] 24%|██▍       | 36/150 [17:03<54:38, 28.76s/it]                                                 24%|██▍       | 36/150 [17:03<54:38, 28.76s/it] 25%|██▍       | 37/150 [17:23<49:23, 26.23s/it]                                                 25%|██▍       | 37/150 [17:23<49:23, 26.23s/it] 25%|██▌       | 38/150 [17:47<47:19, 25.35s/it]                                                 25%|██▌       | 38/150 [17:47<47:19, 25.35s/it] 26%|██▌       | 39/150 [18:07<44:19, 23.96s/it]                                                 26%|██▌       | 39/150 [18:08<44:19, 23.96s/it] 27%|██▋       | 40/150 [18:24<40:10, 21.91s/it]                                                 27%|██▋       | 40/150 [18:25<40:10, 21.91s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 27%|██▋       | 40/150 [18:30<40:10, 21.91s/it]
100%|██████████| 1/1 [00:00<00:00,  2.02it/s][A
                                             [A03/07/2025 21:27:10 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-40
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-40/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-40/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-40/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-40/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-20] due to args.save_total_limit
 27%|██▋       | 41/150 [20:09<1:25:03, 46.82s/it]                                                   27%|██▋       | 41/150 [20:09<1:25:03, 46.82s/it] 28%|██▊       | 42/150 [20:27<1:08:23, 38.00s/it]                                                   28%|██▊       | 42/150 [20:28<1:08:23, 38.00s/it] 29%|██▊       | 43/150 [20:46<57:32, 32.27s/it]                                                   29%|██▊       | 43/150 [20:46<57:32, 32.27s/it] 29%|██▉       | 44/150 [21:06<50:38, 28.66s/it]                                                 29%|██▉       | 44/150 [21:06<50:38, 28.66s/it] 30%|███       | 45/150 [21:21<43:06, 24.63s/it]                                                 30%|███       | 45/150 [21:21<43:06, 24.63s/it] 31%|███       | 46/150 [21:50<44:38, 25.75s/it]                                                 31%|███       | 46/150 [21:50<44:38, 25.75s/it] 31%|███▏      | 47/150 [22:07<39:41, 23.12s/it]                                                 31%|███▏      | 47/150 [22:07<39:41, 23.12s/it] 32%|███▏      | 48/150 [22:23<35:44, 21.02s/it]                                                 32%|███▏      | 48/150 [22:23<35:44, 21.02s/it] 33%|███▎      | 49/150 [22:39<33:03, 19.64s/it]                                                 33%|███▎      | 49/150 [22:39<33:03, 19.64s/it] 33%|███▎      | 50/150 [22:58<32:28, 19.49s/it]                                                 33%|███▎      | 50/150 [22:58<32:28, 19.49s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 33%|███▎      | 50/150 [23:04<32:28, 19.49s/it]
100%|██████████| 1/1 [00:00<00:00,  1.61it/s][A
                                             [A03/07/2025 21:31:44 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-50
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-50/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-50/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-50/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-50/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-30] due to args.save_total_limit
 34%|███▍      | 51/150 [24:29<1:07:12, 40.73s/it]                                                   34%|███▍      | 51/150 [24:29<1:07:12, 40.73s/it] 35%|███▍      | 52/150 [24:47<55:28, 33.97s/it]                                                   35%|███▍      | 52/150 [24:47<55:28, 33.97s/it] 35%|███▌      | 53/150 [25:04<46:38, 28.85s/it]                                                 35%|███▌      | 53/150 [25:04<46:38, 28.85s/it] 36%|███▌      | 54/150 [25:19<39:36, 24.76s/it]                                                 36%|███▌      | 54/150 [25:19<39:36, 24.76s/it] 37%|███▋      | 55/150 [25:41<37:50, 23.90s/it]                                                 37%|███▋      | 55/150 [25:41<37:50, 23.90s/it] 37%|███▋      | 56/150 [26:08<39:15, 25.06s/it]                                                 37%|███▋      | 56/150 [26:09<39:15, 25.06s/it] 38%|███▊      | 57/150 [26:22<33:31, 21.63s/it]                                                 38%|███▊      | 57/150 [26:22<33:31, 21.63s/it] 39%|███▊      | 58/150 [26:39<30:52, 20.13s/it]                                                 39%|███▊      | 58/150 [26:39<30:52, 20.13s/it] 39%|███▉      | 59/150 [27:00<31:06, 20.51s/it]                                                 39%|███▉      | 59/150 [27:01<31:06, 20.51s/it] 40%|████      | 60/150 [27:16<28:28, 18.98s/it]                                                 40%|████      | 60/150 [27:16<28:28, 18.98s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 40%|████      | 60/150 [27:23<28:28, 18.98s/it]
100%|██████████| 1/1 [00:00<00:00,  1.46it/s][A
                                             [A03/07/2025 21:36:03 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-60
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-60/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-60/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-60/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-60/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-40] due to args.save_total_limit
 41%|████      | 61/150 [29:04<1:08:08, 45.93s/it]                                                   41%|████      | 61/150 [29:05<1:08:08, 45.93s/it] 41%|████▏     | 62/150 [29:26<56:45, 38.70s/it]                                                   41%|████▏     | 62/150 [29:27<56:45, 38.70s/it] 42%|████▏     | 63/150 [29:47<48:19, 33.33s/it]                                                 42%|████▏     | 63/150 [29:47<48:19, 33.33s/it] 43%|████▎     | 64/150 [30:04<40:33, 28.29s/it]                                                 43%|████▎     | 64/150 [30:04<40:33, 28.29s/it] 43%|████▎     | 65/150 [30:22<36:00, 25.42s/it]                                                 43%|████▎     | 65/150 [30:22<36:00, 25.42s/it] 44%|████▍     | 66/150 [30:46<34:45, 24.83s/it]                                                 44%|████▍     | 66/150 [30:46<34:45, 24.83s/it] 45%|████▍     | 67/150 [31:06<32:17, 23.35s/it]                                                 45%|████▍     | 67/150 [31:06<32:17, 23.35s/it] 45%|████▌     | 68/150 [31:24<29:47, 21.79s/it]                                                 45%|████▌     | 68/150 [31:24<29:47, 21.79s/it] 46%|████▌     | 69/150 [31:40<27:11, 20.14s/it]                                                 46%|████▌     | 69/150 [31:40<27:11, 20.14s/it] 47%|████▋     | 70/150 [31:58<26:05, 19.56s/it]                                                 47%|████▋     | 70/150 [31:58<26:05, 19.56s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 47%|████▋     | 70/150 [32:04<26:05, 19.56s/it]
100%|██████████| 1/1 [00:00<00:00,  1.16it/s][A
                                             [A03/07/2025 21:40:44 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-70
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-70/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-70/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-70/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-70/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-60] due to args.save_total_limit
 47%|████▋     | 71/150 [33:33<55:26, 42.11s/it]                                                 47%|████▋     | 71/150 [33:33<55:26, 42.11s/it] 48%|████▊     | 72/150 [33:52<45:45, 35.19s/it]                                                 48%|████▊     | 72/150 [33:52<45:45, 35.19s/it] 49%|████▊     | 73/150 [34:08<37:49, 29.48s/it]                                                 49%|████▊     | 73/150 [34:08<37:49, 29.48s/it] 49%|████▉     | 74/150 [34:23<31:49, 25.12s/it]                                                 49%|████▉     | 74/150 [34:23<31:49, 25.12s/it] 50%|█████     | 75/150 [34:49<31:31, 25.21s/it]                                                 50%|█████     | 75/150 [34:49<31:31, 25.21s/it] 51%|█████     | 76/150 [35:15<31:32, 25.57s/it]                                                 51%|█████     | 76/150 [35:15<31:32, 25.57s/it] 51%|█████▏    | 77/150 [35:37<29:38, 24.36s/it]                                                 51%|█████▏    | 77/150 [35:37<29:38, 24.36s/it] 52%|█████▏    | 78/150 [35:57<27:40, 23.06s/it]                                                 52%|█████▏    | 78/150 [35:57<27:40, 23.06s/it] 53%|█████▎    | 79/150 [36:11<24:19, 20.56s/it]                                                 53%|█████▎    | 79/150 [36:11<24:19, 20.56s/it] 53%|█████▎    | 80/150 [36:30<23:23, 20.05s/it]                                                 53%|█████▎    | 80/150 [36:30<23:23, 20.05s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 53%|█████▎    | 80/150 [36:37<23:23, 20.05s/it]
100%|██████████| 1/1 [00:01<00:00,  1.09s/it][A
                                             [A03/07/2025 21:45:17 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-80
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-80/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-80/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-80/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-80/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-70] due to args.save_total_limit
 54%|█████▍    | 81/150 [38:04<48:39, 42.31s/it]                                                 54%|█████▍    | 81/150 [38:05<48:39, 42.31s/it] 55%|█████▍    | 82/150 [38:25<40:39, 35.88s/it]                                                 55%|█████▍    | 82/150 [38:25<40:39, 35.88s/it] 55%|█████▌    | 83/150 [38:47<35:21, 31.66s/it]                                                 55%|█████▌    | 83/150 [38:47<35:21, 31.66s/it] 56%|█████▌    | 84/150 [39:08<31:23, 28.54s/it]                                                 56%|█████▌    | 84/150 [39:08<31:23, 28.54s/it] 57%|█████▋    | 85/150 [39:23<26:31, 24.49s/it]                                                 57%|█████▋    | 85/150 [39:23<26:31, 24.49s/it] 57%|█████▋    | 86/150 [39:49<26:20, 24.70s/it]                                                 57%|█████▋    | 86/150 [39:49<26:20, 24.70s/it] 58%|█████▊    | 87/150 [40:07<24:01, 22.88s/it]                                                 58%|█████▊    | 87/150 [40:07<24:01, 22.88s/it] 59%|█████▊    | 88/150 [40:22<21:17, 20.60s/it]                                                 59%|█████▊    | 88/150 [40:23<21:17, 20.60s/it] 59%|█████▉    | 89/150 [40:42<20:38, 20.30s/it]                                                 59%|█████▉    | 89/150 [40:42<20:38, 20.30s/it] 60%|██████    | 90/150 [41:01<19:50, 19.84s/it]                                                 60%|██████    | 90/150 [41:01<19:50, 19.84s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 60%|██████    | 90/150 [41:07<19:50, 19.84s/it]
100%|██████████| 1/1 [00:00<00:00,  1.18it/s][A
                                             [A03/07/2025 21:49:47 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-90
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-90/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-90/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-90/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-90/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-80] due to args.save_total_limit
 61%|██████    | 91/150 [42:35<41:19, 42.02s/it]                                                 61%|██████    | 91/150 [42:35<41:19, 42.02s/it] 61%|██████▏   | 92/150 [42:52<33:24, 34.56s/it]                                                 61%|██████▏   | 92/150 [42:52<33:24, 34.56s/it] 62%|██████▏   | 93/150 [43:10<28:12, 29.69s/it]                                                 62%|██████▏   | 93/150 [43:10<28:12, 29.69s/it] 63%|██████▎   | 94/150 [43:28<24:23, 26.13s/it]                                                 63%|██████▎   | 94/150 [43:28<24:23, 26.13s/it] 63%|██████▎   | 95/150 [43:45<21:23, 23.35s/it]                                                 63%|██████▎   | 95/150 [43:45<21:23, 23.35s/it] 64%|██████▍   | 96/150 [44:13<22:25, 24.92s/it]                                                 64%|██████▍   | 96/150 [44:13<22:25, 24.92s/it] 65%|██████▍   | 97/150 [44:32<20:25, 23.12s/it]                                                 65%|██████▍   | 97/150 [44:32<20:25, 23.12s/it] 65%|██████▌   | 98/150 [44:54<19:33, 22.57s/it]                                                 65%|██████▌   | 98/150 [44:54<19:33, 22.57s/it] 66%|██████▌   | 99/150 [45:15<18:48, 22.13s/it]                                                 66%|██████▌   | 99/150 [45:15<18:48, 22.13s/it] 67%|██████▋   | 100/150 [45:42<19:48, 23.77s/it]                                                  67%|██████▋   | 100/150 [45:43<19:48, 23.77s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 67%|██████▋   | 100/150 [45:48<19:48, 23.77s/it]
100%|██████████| 1/1 [00:00<00:00,  1.13it/s][A
                                             [A03/07/2025 21:54:28 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-100
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-100/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-100/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-100/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-100/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-50] due to args.save_total_limit
 67%|██████▋   | 101/150 [47:13<35:49, 43.86s/it]                                                  67%|██████▋   | 101/150 [47:13<35:49, 43.86s/it] 68%|██████▊   | 102/150 [47:35<29:43, 37.16s/it]                                                  68%|██████▊   | 102/150 [47:35<29:43, 37.16s/it] 69%|██████▊   | 103/150 [47:55<25:05, 32.04s/it]                                                  69%|██████▊   | 103/150 [47:55<25:05, 32.04s/it] 69%|██████▉   | 104/150 [48:15<21:52, 28.53s/it]                                                  69%|██████▉   | 104/150 [48:15<21:52, 28.53s/it] 70%|███████   | 105/150 [48:30<18:27, 24.60s/it]                                                  70%|███████   | 105/150 [48:31<18:27, 24.60s/it] 71%|███████   | 106/150 [48:56<18:13, 24.86s/it]                                                  71%|███████   | 106/150 [48:56<18:13, 24.86s/it] 71%|███████▏  | 107/150 [49:21<17:54, 24.99s/it]                                                  71%|███████▏  | 107/150 [49:21<17:54, 24.99s/it] 72%|███████▏  | 108/150 [49:42<16:42, 23.86s/it]                                                  72%|███████▏  | 108/150 [49:43<16:42, 23.86s/it] 73%|███████▎  | 109/150 [50:05<15:59, 23.40s/it]                                                  73%|███████▎  | 109/150 [50:05<15:59, 23.40s/it] 73%|███████▎  | 110/150 [50:21<14:06, 21.17s/it]                                                  73%|███████▎  | 110/150 [50:21<14:06, 21.17s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 73%|███████▎  | 110/150 [50:28<14:06, 21.17s/it]
100%|██████████| 1/1 [00:00<00:00,  1.60it/s][A
                                             [A03/07/2025 21:59:07 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-110
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-110/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-110/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-110/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-110/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-90] due to args.save_total_limit
 74%|███████▍  | 111/150 [51:50<27:05, 41.68s/it]                                                  74%|███████▍  | 111/150 [51:51<27:05, 41.68s/it] 75%|███████▍  | 112/150 [52:13<22:49, 36.03s/it]                                                  75%|███████▍  | 112/150 [52:13<22:49, 36.03s/it] 75%|███████▌  | 113/150 [52:33<19:19, 31.35s/it]                                                  75%|███████▌  | 113/150 [52:34<19:19, 31.35s/it] 76%|███████▌  | 114/150 [52:58<17:34, 29.29s/it]                                                  76%|███████▌  | 114/150 [52:58<17:34, 29.29s/it] 77%|███████▋  | 115/150 [53:17<15:13, 26.09s/it]                                                  77%|███████▋  | 115/150 [53:17<15:13, 26.09s/it] 77%|███████▋  | 116/150 [53:41<14:28, 25.55s/it]                                                  77%|███████▋  | 116/150 [53:41<14:28, 25.55s/it] 78%|███████▊  | 117/150 [54:05<13:45, 25.01s/it]                                                  78%|███████▊  | 117/150 [54:05<13:45, 25.01s/it] 79%|███████▊  | 118/150 [54:24<12:25, 23.30s/it]                                                  79%|███████▊  | 118/150 [54:24<12:25, 23.30s/it] 79%|███████▉  | 119/150 [54:40<10:55, 21.15s/it]                                                  79%|███████▉  | 119/150 [54:40<10:55, 21.15s/it] 80%|████████  | 120/150 [54:57<09:55, 19.85s/it]                                                  80%|████████  | 120/150 [54:57<09:55, 19.85s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 80%|████████  | 120/150 [55:03<09:55, 19.85s/it]
100%|██████████| 1/1 [00:00<00:00,  1.25it/s][A
                                             [A03/07/2025 22:03:42 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-120
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-120/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-120/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-120/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-120/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-100] due to args.save_total_limit
 81%|████████  | 121/150 [55:37<12:31, 25.91s/it]                                                  81%|████████  | 121/150 [55:37<12:31, 25.91s/it] 81%|████████▏ | 122/150 [55:52<10:36, 22.72s/it]                                                  81%|████████▏ | 122/150 [55:52<10:36, 22.72s/it] 82%|████████▏ | 123/150 [56:18<10:38, 23.65s/it]                                                  82%|████████▏ | 123/150 [56:18<10:38, 23.65s/it] 83%|████████▎ | 124/150 [56:36<09:33, 22.07s/it]                                                  83%|████████▎ | 124/150 [56:37<09:33, 22.07s/it] 83%|████████▎ | 125/150 [56:54<08:40, 20.83s/it]                                                  83%|████████▎ | 125/150 [56:55<08:40, 20.83s/it] 84%|████████▍ | 126/150 [57:25<09:27, 23.63s/it]                                                  84%|████████▍ | 126/150 [57:25<09:27, 23.63s/it] 85%|████████▍ | 127/150 [57:41<08:17, 21.62s/it]                                                  85%|████████▍ | 127/150 [57:42<08:17, 21.62s/it] 85%|████████▌ | 128/150 [58:00<07:34, 20.64s/it]                                                  85%|████████▌ | 128/150 [58:00<07:34, 20.64s/it] 86%|████████▌ | 129/150 [58:21<07:17, 20.85s/it]                                                  86%|████████▌ | 129/150 [58:21<07:17, 20.85s/it] 87%|████████▋ | 130/150 [58:39<06:38, 19.92s/it]                                                  87%|████████▋ | 130/150 [58:39<06:38, 19.92s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 87%|████████▋ | 130/150 [58:45<06:38, 19.92s/it]
100%|██████████| 1/1 [00:00<00:00,  1.85it/s][A
                                             [A03/07/2025 22:07:24 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-130
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-130/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-130/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-130/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-130/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-120] due to args.save_total_limit
 87%|████████▋ | 131/150 [1:00:08<12:50, 40.54s/it]                                                    87%|████████▋ | 131/150 [1:00:08<12:50, 40.54s/it] 88%|████████▊ | 132/150 [1:00:29<10:24, 34.69s/it]                                                    88%|████████▊ | 132/150 [1:00:29<10:24, 34.69s/it] 89%|████████▊ | 133/150 [1:00:46<08:20, 29.42s/it]                                                    89%|████████▊ | 133/150 [1:00:46<08:20, 29.42s/it] 89%|████████▉ | 134/150 [1:01:04<06:57, 26.09s/it]                                                    89%|████████▉ | 134/150 [1:01:05<06:57, 26.09s/it] 90%|█████████ | 135/150 [1:01:25<06:05, 24.39s/it]                                                    90%|█████████ | 135/150 [1:01:25<06:05, 24.39s/it] 91%|█████████ | 136/150 [1:01:53<05:59, 25.68s/it]                                                    91%|█████████ | 136/150 [1:01:53<05:59, 25.68s/it] 91%|█████████▏| 137/150 [1:02:07<04:47, 22.10s/it]                                                    91%|█████████▏| 137/150 [1:02:07<04:47, 22.10s/it] 92%|█████████▏| 138/150 [1:02:32<04:36, 23.02s/it]                                                    92%|█████████▏| 138/150 [1:02:32<04:36, 23.02s/it] 93%|█████████▎| 139/150 [1:02:51<03:59, 21.77s/it]                                                    93%|█████████▎| 139/150 [1:02:51<03:59, 21.77s/it] 93%|█████████▎| 140/150 [1:03:07<03:19, 19.96s/it]                                                    93%|█████████▎| 140/150 [1:03:07<03:19, 19.96s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A 93%|█████████▎| 140/150 [1:03:13<03:19, 19.96s/it]
100%|██████████| 1/1 [00:00<00:00,  1.58it/s][A
                                             [A03/07/2025 22:11:53 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-140
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-140/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-140/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-140/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-140/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-130] due to args.save_total_limit
 94%|█████████▍| 141/150 [1:04:38<06:11, 41.25s/it]                                                    94%|█████████▍| 141/150 [1:04:38<06:11, 41.25s/it] 95%|█████████▍| 142/150 [1:05:01<04:47, 35.98s/it]                                                    95%|█████████▍| 142/150 [1:05:02<04:47, 35.98s/it] 95%|█████████▌| 143/150 [1:05:18<03:30, 30.12s/it]                                                    95%|█████████▌| 143/150 [1:05:18<03:30, 30.12s/it] 96%|█████████▌| 144/150 [1:05:36<02:38, 26.45s/it]                                                    96%|█████████▌| 144/150 [1:05:36<02:38, 26.45s/it] 97%|█████████▋| 145/150 [1:05:54<01:59, 23.98s/it]                                                    97%|█████████▋| 145/150 [1:05:54<01:59, 23.98s/it] 97%|█████████▋| 146/150 [1:06:19<01:37, 24.39s/it]                                                    97%|█████████▋| 146/150 [1:06:19<01:37, 24.39s/it] 98%|█████████▊| 147/150 [1:06:34<01:04, 21.38s/it]                                                    98%|█████████▊| 147/150 [1:06:34<01:04, 21.38s/it] 99%|█████████▊| 148/150 [1:06:57<00:44, 22.02s/it]                                                    99%|█████████▊| 148/150 [1:06:57<00:44, 22.02s/it] 99%|█████████▉| 149/150 [1:07:16<00:21, 21.05s/it]                                                    99%|█████████▉| 149/150 [1:07:16<00:21, 21.05s/it]100%|██████████| 150/150 [1:07:36<00:00, 20.76s/it]                                                   100%|██████████| 150/150 [1:07:36<00:00, 20.76s/it]***** Running Evaluation *****
  Num examples = 253
  Batch size = 48

  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A100%|██████████| 150/150 [1:07:42<00:00, 20.76s/it]
100%|██████████| 1/1 [00:01<00:00,  1.06s/it][A
                                             [A03/07/2025 22:16:22 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-150
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-150/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-150/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-150/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-150/special_tokens_map.json
Deleting older checkpoint [/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/checkpoint-140] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 150/150 [1:08:58<00:00, 20.76s/it]100%|██████████| 150/150 [1:08:58<00:00, 27.59s/it]
03/07/2025 22:17:37 - INFO - openmatch.trainer.dense_trainer -   Saving model checkpoint to /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty
Configuration saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/config.json
Model weights saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/pytorch_model.bin
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/special_tokens_map.json
tokenizer config file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/tokenizer_config.json
Special tokens file saved in /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-beauty/special_tokens_map.json
wandb: - 0.016 MB of 0.016 MB uploadedwandb: \ 0.016 MB of 0.016 MB uploadedwandb: | 0.016 MB of 0.016 MB uploadedwandb: / 0.016 MB of 0.016 MB uploadedwandb: - 0.016 MB of 0.016 MB uploadedwandb: \ 0.322 MB of 0.901 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▂▂▂▂▃▂▂▂▁▂▃▆▅
wandb:                   eval/runtime ▂▆▄▁▃█▃▅▃▂▆▂▂▄▆
wandb:        eval/samples_per_second ▇▃▄█▅▁▅▄▅▇▂▆▇▅▃
wandb:          eval/steps_per_second ▇▃▄█▅▁▅▄▅▇▂▆▇▄▃
wandb:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/learning_rate ▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:                     train/loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 6.23846
wandb:                   eval/runtime 6.4214
wandb:        eval/samples_per_second 7.475
wandb:          eval/steps_per_second 0.156
wandb:                    train/epoch 30.0
wandb:              train/global_step 150
wandb:            train/learning_rate 0.0
wandb:                     train/loss 5.9708
wandb:               train/total_flos 0.0
wandb:               train/train_loss 11.66244
wandb:            train/train_runtime 4148.1328
wandb: train/train_samples_per_second 12.844
wandb:   train/train_steps_per_second 0.036
wandb: 
wandb: 🚀 View run TASTE_amzn-beauty at: https://wandb.ai/jingyuanhe1222/RecSys-Benchmark/runs/2z0d5h9c
wandb: ⭐️ View project at: https://wandb.ai/jingyuanhe1222/RecSys-Benchmark
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250307_210832-2z0d5h9c/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
