W0414 16:08:42.223308 22783400301568 torch/distributed/run.py:779] 
W0414 16:08:42.223308 22783400301568 torch/distributed/run.py:779] *****************************************
W0414 16:08:42.223308 22783400301568 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0414 16:08:42.223308 22783400301568 torch/distributed/run.py:779] *****************************************
04/14/2025 16:08:51 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
04/14/2025 16:08:51 - INFO - __main__ -   Training/evaluation parameters DRTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
distil_mode=pairwise,
distillation=False,
do_encode=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=2500,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gc_p_chunk_size=32,
gc_q_chunk_size=4,
grad_cache=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0004,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-books/runs/Apr14_16-08-50_babel-9-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
negatives_x_device=False,
no_cuda=False,
num_train_epochs=30.0,
optim=adamw_hf,
output_dir=/data/group_data/cx_group/REC/checkpoints/TASTE_amzn-books,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=48,
per_device_train_batch_size=48,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=TASTE_amzn-books,
save_on_each_node=False,
save_steps=2500,
save_strategy=steps,
save_total_limit=2,
seed=2022,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mapping_dataset=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/14/2025 16:08:51 - INFO - __main__ -   MODEL parameters ModelArguments(model_name_or_path='google-t5/t5-base', target_model_path=None, config_name=None, tokenizer_name=None, processor_name=None, cache_dir=None, untie_encoder=False, feature='last_hidden_state', pooling='first', add_linear_head=False, projection_in_dim=768, projection_out_dim=768, dtype='float32', encoder_only=False, pos_token=None, neg_token=None, normalize=False, param_efficient_method=None)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
04/14/2025 16:08:52 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
04/14/2025 16:08:52 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
04/14/2025 16:08:52 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
04/14/2025 16:08:52 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, 16-bits training: False
04/14/2025 16:08:52 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
04/14/2025 16:08:52 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
04/14/2025 16:08:53 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, 16-bits training: False
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
Loading model from /data/group_data/cx_group/REC/checkpoints/TASTE_amzn-books/checkpoint-32500.
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:1953: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location="cpu")
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
***** Running training *****
  Num examples = 7159187
  Num Epochs = 30
  Instantaneous batch size per device = 48
  Total train batch size (w. parallel, distributed & accumulation) = 384
  Gradient Accumulation steps = 1
  Total optimization steps = 559320
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 32500
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: jingyuanhe1222. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/jingyuah/REC_source/TASTE/reproduce/train/books/wandb/run-20250414_162530-s9c9n7wo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TASTE_amzn-books
wandb: ⭐️ View project at https://wandb.ai/jingyuanhe1222/RecSys-Benchmark
wandb: 🚀 View run at https://wandb.ai/jingyuanhe1222/RecSys-Benchmark/runs/s9c9n7wo
  0%|          | 0/559320 [00:00<?, ?it/s]/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
/home/jingyuah/miniconda3/envs/taste/lib/python3.8/site-packages/transformers/trainer.py:2095: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
[rank1]:[W414 16:26:15.927072445 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W414 16:26:22.065214021 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W414 16:26:25.852314332 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W414 16:26:29.117484671 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W414 16:26:30.509303431 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W414 16:26:34.768431058 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W414 16:26:36.504359363 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W414 16:26:38.024630358 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  6%|▌         | 32501/559320 [01:03<17:02, 515.29it/s]  6%|▌         | 32504/559320 [01:20<17:02, 515.29it/s]  6%|▌         | 32505/559320 [01:23<24:49, 353.76it/s]  6%|▌         | 32506/559320 [01:26<26:26, 332.13it/s]  6%|▌         | 32507/559320 [01:30<30:05, 291.75it/s]  6%|▌         | 32508/559320 [01:33<33:23, 262.95it/s]  6%|▌         | 32509/559320 [01:38<41:41, 210.63it/s]  6%|▌         | 32510/559320 [01:41<48:24, 181.39it/s]  6%|▌         | 32511/559320 [01:46<1:03:32, 138.19it/s]  6%|▌         | 32512/559320 [01:49<1:16:19, 115.05it/s]  6%|▌         | 32513/559320 [02:01<2:35:06, 56.61it/s]   6%|▌         | 32514/559320 [02:04<3:04:19, 47.63it/s]  6%|▌         | 32515/559320 [02:09<4:16:31, 34.23it/s]  6%|▌         | 32516/559320 [02:12<5:14:45, 27.89it/s]  6%|▌         | 32517/559320 [02:17<7:44:48, 18.89it/s]  6%|▌         | 32518/559320 [02:23<11:04:40, 13.21it/s]  6%|▌         | 32519/559320 [02:26<13:50:26, 10.57it/s]  6%|▌         | 32520/559320 [02:35<25:21:51,  5.77it/s]  6%|▌         | 32521/559320 [02:37<30:28:44,  4.80it/s]  6%|▌         | 32522/559320 [02:43<43:57:40,  3.33it/s]  6%|▌         | 32523/559320 [02:46<53:46:09,  2.72it/s]  6%|▌         | 32524/559320 [02:52<80:29:32,  1.82it/s]  6%|▌         | 32525/559320 [02:55<97:51:48,  1.50it/s]  6%|▌         | 32526/559320 [03:00<141:41:25,  1.03it/s]  6%|▌         | 32527/559320 [03:03<166:25:43,  1.14s/it]  6%|▌         | 32528/559320 [03:12<279:46:31,  1.91s/it]  6%|▌         | 32529/559320 [03:15<301:05:39,  2.06s/it]  6%|▌         | 32530/559320 [03:20<385:22:11,  2.63s/it]  6%|▌         | 32531/559320 [03:24<402:42:45,  2.75s/it]  6%|▌         | 32532/559320 [03:29<482:14:23,  3.30s/it]  6%|▌         | 32533/559320 [03:35<551:05:24,  3.77s/it]  6%|▌         | 32534/559320 [03:38<524:47:23,  3.59s/it]  6%|▌         | 32535/559320 [03:46<691:22:39,  4.72s/it]  6%|▌         | 32536/559320 [03:49<625:51:45,  4.28s/it]  6%|▌         | 32537/559320 [03:54<677:45:52,  4.63s/it]  6%|▌         | 32538/559320 [03:57<614:23:26,  4.20s/it]  6%|▌         | 32539/559320 [04:03<668:23:54,  4.57s/it]  6%|▌         | 32540/559320 [04:06<601:38:27,  4.11s/it]  6%|▌         | 32541/559320 [04:11<660:54:11,  4.52s/it]  6%|▌         | 32542/559320 [04:14<592:48:53,  4.05s/it]  6%|▌         | 32543/559320 [04:19<617:42:50,  4.22s/it]  6%|▌         | 32544/559320 [04:22<564:37:37,  3.86s/it]  6%|▌         | 32545/559320 [04:30<772:05:26,  5.28s/it]  6%|▌         | 32546/559320 [04:35<756:11:41,  5.17s/it]  6%|▌         | 32547/559320 [04:38<659:27:11,  4.51s/it]  6%|▌         | 32548/559320 [04:44<700:44:26,  4.79s/it]  6%|▌         | 32549/559320 [04:47<619:26:32,  4.23s/it]  6%|▌         | 32550/559320 [04:54<768:30:16,  5.25s/it]  6%|▌         | 32551/559320 [04:57<667:07:12,  4.56s/it]  6%|▌         | 32552/559320 [05:04<785:45:30,  5.37s/it]  6%|▌         | 32553/559320 [05:07<680:23:23,  4.65s/it]  6%|▌         | 32554/559320 [05:13<710:02:46,  4.85s/it]  6%|▌         | 32555/559320 [05:16<625:16:16,  4.27s/it]  6%|▌         | 32556/559320 [05:20<642:00:43,  4.39s/it]  6%|▌         | 32557/559320 [05:23<577:50:21,  3.95s/it]  6%|▌         | 32558/559320 [05:31<730:43:16,  4.99s/it]  6%|▌         | 32559/559320 [05:34<641:48:09,  4.39s/it]  6%|▌         | 32560/559320 [05:42<803:45:04,  5.49s/it]  6%|▌         | 32561/559320 [05:47<772:32:12,  5.28s/it]  6%|▌         | 32562/559320 [05:49<662:14:11,  4.53s/it]  6%|▌         | 32563/559320 [05:54<670:03:45,  4.58s/it]  6%|▌         | 32564/559320 [05:57<597:47:35,  4.09s/it]  6%|▌         | 32565/559320 [06:02<649:44:59,  4.44s/it]  6%|▌         | 32566/559320 [06:05<586:38:30,  4.01s/it]  6%|▌         | 32567/559320 [06:13<741:35:55,  5.07s/it]  6%|▌         | 32568/559320 [06:16<648:31:12,  4.43s/it]  6%|▌         | 32569/559320 [06:21<688:12:47,  4.70s/it]  6%|▌         | 32570/559320 [06:24<612:09:06,  4.18s/it]  6%|▌         | 32571/559320 [06:29<635:22:17,  4.34s/it]  6%|▌         | 32572/559320 [06:31<566:18:57,  3.87s/it]  6%|▌         | 32573/559320 [06:37<658:58:57,  4.50s/it]  6%|▌         | 32574/559320 [06:45<807:48:13,  5.52s/it]  6%|▌         | 32575/559320 [06:48<693:17:36,  4.74s/it]  6%|▌         | 32576/559320 [06:53<698:00:01,  4.77s/it]  6%|▌         | 32577/559320 [06:56<619:56:44,  4.24s/it]  6%|▌         | 32578/559320 [06:59<562:56:29,  3.85s/it]  6%|▌         | 32579/559320 [07:04<630:12:23,  4.31s/it]  6%|▌         | 32580/559320 [07:10<675:07:46,  4.61s/it]  6%|▌         | 32581/559320 [07:13<604:40:17,  4.13s/it]  6%|▌         | 32582/559320 [07:16<555:18:03,  3.80s/it]  6%|▌         | 32583/559320 [07:23<700:53:02,  4.79s/it]  6%|▌         | 32584/559320 [07:29<779:45:46,  5.33s/it]  6%|▌         | 32585/559320 [07:32<675:37:40,  4.62s/it]  6%|▌         | 32586/559320 [07:37<675:35:50,  4.62s/it]  6%|▌         | 32587/559320 [07:40<596:18:46,  4.08s/it]  6%|▌         | 32588/559320 [07:45<658:47:52,  4.50s/it]  6%|▌         | 32589/559320 [07:48<593:34:11,  4.06s/it]  6%|▌         | 32590/559320 [07:56<762:18:29,  5.21s/it]  6%|▌         | 32591/559320 [07:59<663:39:52,  4.54s/it]  6%|▌         | 32592/559320 [08:05<700:00:38,  4.78s/it]  6%|▌         | 32593/559320 [08:08<619:58:56,  4.24s/it]  6%|▌         | 32594/559320 [08:12<639:32:12,  4.37s/it]  6%|▌         | 32595/559320 [08:15<576:55:17,  3.94s/it]  6%|▌         | 32596/559320 [08:21<645:22:18,  4.41s/it]  6%|▌         | 32597/559320 [08:24<582:09:54,  3.98s/it]  6%|▌         | 32598/559320 [08:33<821:29:55,  5.61s/it]  6%|▌         | 32599/559320 [08:36<704:55:31,  4.82s/it]  6%|▌         | 32600/559320 [08:42<757:51:30,  5.18s/it]                                                            6%|▌         | 32600/559320 [08:42<757:51:30,  5.18s/it]  6%|▌         | 32601/559320 [08:45<663:06:42,  4.53s/it]  6%|▌         | 32602/559320 [08:50<700:45:21,  4.79s/it]  6%|▌         | 32603/559320 [08:53<612:51:07,  4.19s/it]  6%|▌         | 32604/559320 [08:59<663:08:24,  4.53s/it]  6%|▌         | 32605/559320 [09:02<594:36:46,  4.06s/it]  6%|▌         | 32606/559320 [09:08<713:11:46,  4.87s/it]  6%|▌         | 32607/559320 [09:11<621:12:14,  4.25s/it]  6%|▌         | 32608/559320 [09:17<710:40:57,  4.86s/it]  6%|▌         | 32609/559320 [09:20<628:52:55,  4.30s/it]  6%|▌         | 32610/559320 [09:26<680:40:07,  4.65s/it]  6%|▌         | 32611/559320 [09:31<681:36:09,  4.66s/it]  6%|▌         | 32612/559320 [09:34<609:58:02,  4.17s/it]  6%|▌         | 32613/559320 [09:43<839:32:57,  5.74s/it]  6%|▌         | 32614/559320 [09:46<716:41:49,  4.90s/it]  6%|▌         | 32615/559320 [09:53<791:59:23,  5.41s/it]  6%|▌         | 32616/559320 [09:58<773:39:42,  5.29s/it]  6%|▌         | 32617/559320 [10:00<669:53:51,  4.58s/it]  6%|▌         | 32618/559320 [10:05<670:21:30,  4.58s/it]  6%|▌         | 32619/559320 [10:08<591:18:45,  4.04s/it]  6%|▌         | 32620/559320 [10:15<710:15:01,  4.85s/it]  6%|▌         | 32621/559320 [10:18<627:14:54,  4.29s/it]  6%|▌         | 32622/559320 [10:25<760:31:35,  5.20s/it]  6%|▌         | 32623/559320 [10:28<648:37:16,  4.43s/it]  6%|▌         | 32624/559320 [10:33<702:01:19,  4.80s/it]  6%|▌         | 32625/559320 [10:39<732:34:53,  5.01s/it]  6%|▌         | 32626/559320 [10:42<641:09:05,  4.38s/it]  6%|▌         | 32627/559320 [10:49<792:23:47,  5.42s/it]  6%|▌         | 32628/559320 [10:52<684:53:13,  4.68s/it]  6%|▌         | 32629/559320 [11:00<808:55:03,  5.53s/it]  6%|▌         | 32630/559320 [11:03<689:45:21,  4.71s/it]  6%|▌         | 32631/559320 [11:08<729:27:06,  4.99s/it]  6%|▌         | 32632/559320 [11:11<642:41:34,  4.39s/it]  6%|▌         | 32633/559320 [11:16<661:43:11,  4.52s/it]  6%|▌         | 32634/559320 [11:19<593:47:18,  4.06s/it]  6%|▌         | 32635/559320 [11:24<626:43:19,  4.28s/it]  6%|▌         | 32636/559320 [11:27<560:33:06,  3.83s/it]  6%|▌         | 32637/559320 [11:34<699:44:27,  4.78s/it]  6%|▌         | 32638/559320 [11:36<611:24:20,  4.18s/it]  6%|▌         | 32639/559320 [11:44<745:08:18,  5.09s/it]  6%|▌         | 32640/559320 [11:47<644:18:41,  4.40s/it]  6%|▌         | 32641/559320 [11:51<662:31:16,  4.53s/it]  6%|▌         | 32642/559320 [11:54<585:03:04,  4.00s/it]  6%|▌         | 32643/559320 [12:03<802:39:46,  5.49s/it]  6%|▌         | 32644/559320 [12:06<689:50:50,  4.72s/it]  6%|▌         | 32645/559320 [12:13<783:49:01,  5.36s/it]  6%|▌         | 32646/559320 [12:16<682:17:38,  4.66s/it]  6%|▌         | 32647/559320 [12:21<684:51:18,  4.68s/it]  6%|▌         | 32648/559320 [12:23<602:16:00,  4.12s/it]  6%|▌         | 32649/559320 [12:28<625:45:37,  4.28s/it]  6%|▌         | 32650/559320 [12:31<570:08:31,  3.90s/it]  6%|▌         | 32651/559320 [12:38<719:55:20,  4.92s/it]  6%|▌         | 32652/559320 [12:41<634:35:01,  4.34s/it]  6%|▌         | 32653/559320 [12:49<767:39:50,  5.25s/it]  6%|▌         | 32654/559320 [12:52<666:28:39,  4.56s/it]  6%|▌         | 32655/559320 [12:57<710:39:41,  4.86s/it]  6%|▌         | 32656/559320 [13:00<628:08:00,  4.29s/it]  6%|▌         | 32657/559320 [13:05<651:44:26,  4.45s/it]  6%|▌         | 32658/559320 [13:10<662:32:48,  4.53s/it]  6%|▌         | 32659/559320 [13:13<594:10:13,  4.06s/it]  6%|▌         | 32660/559320 [13:21<789:59:49,  5.40s/it]  6%|▌         | 32661/559320 [13:24<674:45:44,  4.61s/it]  6%|▌         | 32662/559320 [13:30<756:15:57,  5.17s/it]  6%|▌         | 32663/559320 [13:33<661:05:48,  4.52s/it]  6%|▌         | 32664/559320 [13:39<704:22:18,  4.81s/it]  6%|▌         | 32665/559320 [13:42<624:19:26,  4.27s/it]  6%|▌         | 32666/559320 [13:49<749:19:31,  5.12s/it]  6%|▌         | 32667/559320 [13:55<780:14:49,  5.33s/it]  6%|▌         | 32668/559320 [14:02<853:53:15,  5.84s/it]  6%|▌         | 32669/559320 [14:05<729:38:34,  4.99s/it]  6%|▌         | 32670/559320 [14:12<811:43:54,  5.55s/it]  6%|▌         | 32671/559320 [14:15<699:12:13,  4.78s/it]  6%|▌         | 32672/559320 [14:20<701:47:10,  4.80s/it]  6%|▌         | 32673/559320 [14:22<614:17:34,  4.20s/it]  6%|▌         | 32674/559320 [14:31<800:36:29,  5.47s/it]  6%|▌         | 32675/559320 [14:34<689:04:20,  4.71s/it]  6%|▌         | 32676/559320 [14:40<769:23:44,  5.26s/it]  6%|▌         | 32677/559320 [14:43<661:23:16,  4.52s/it]  6%|▌         | 32678/559320 [14:50<744:17:33,  5.09s/it]  6%|▌         | 32679/559320 [14:53<654:18:19,  4.47s/it]  6%|▌         | 32680/559320 [14:58<687:44:58,  4.70s/it]  6%|▌         | 32681/559320 [15:01<610:22:07,  4.17s/it]  6%|▌         | 32682/559320 [15:10<850:24:49,  5.81s/it]  6%|▌         | 32683/559320 [15:13<725:30:52,  4.96s/it]  6%|▌         | 32684/559320 [15:22<874:57:34,  5.98s/it]  6%|▌         | 32685/559320 [15:25<745:20:01,  5.09s/it]  6%|▌         | 32686/559320 [15:31<802:12:02,  5.48s/it]  6%|▌         | 32687/559320 [15:34<695:31:09,  4.75s/it]  6%|▌         | 32688/559320 [15:40<738:25:00,  5.05s/it]  6%|▌         | 32689/559320 [15:43<646:43:54,  4.42s/it]  6%|▌         | 32690/559320 [15:48<691:32:12,  4.73s/it]  6%|▌         | 32691/559320 [15:51<615:46:39,  4.21s/it]  6%|▌         | 32692/559320 [16:00<796:21:44,  5.44s/it]  6%|▌         | 32693/559320 [16:03<687:54:09,  4.70s/it]  6%|▌         | 32694/559320 [16:11<831:26:01,  5.68s/it]  6%|▌         | 32695/559320 [16:14<714:38:35,  4.89s/it]  6%|▌         | 32696/559320 [16:19<732:11:48,  5.01s/it]  6%|▌         | 32697/559320 [16:22<634:14:49,  4.34s/it]  6%|▌         | 32698/559320 [16:26<651:25:14,  4.45s/it]  6%|▌         | 32699/559320 [16:29<579:34:14,  3.96s/it]  6%|▌         | 32700/559320 [16:39<833:19:57,  5.70s/it]                                                            6%|▌         | 32700/559320 [16:39<833:19:57,  5.70s/it]  6%|▌         | 32701/559320 [16:42<717:06:32,  4.90s/it]  6%|▌         | 32702/559320 [16:47<703:52:45,  4.81s/it]  6%|▌         | 32703/559320 [16:53<775:50:47,  5.30s/it]  6%|▌         | 32704/559320 [16:56<675:19:33,  4.62s/it]  6%|▌         | 32705/559320 [17:02<717:22:46,  4.90s/it]  6%|▌         | 32706/559320 [17:05<634:00:22,  4.33s/it]  6%|▌         | 32707/559320 [17:12<765:26:46,  5.23s/it]  6%|▌         | 32708/559320 [17:15<665:34:02,  4.55s/it]  6%|▌         | 32709/559320 [17:20<695:23:31,  4.75s/it]  6%|▌         | 32710/559320 [17:26<730:25:02,  4.99s/it]  6%|▌         | 32711/559320 [17:31<751:36:21,  5.14s/it]  6%|▌         | 32712/559320 [17:34<658:05:36,  4.50s/it]  6%|▌         | 32713/559320 [17:39<668:38:20,  4.57s/it]  6%|▌         | 32714/559320 [17:42<599:27:57,  4.10s/it]  6%|▌         | 32715/559320 [17:50<790:19:15,  5.40s/it]  6%|▌         | 32716/559320 [17:55<763:22:22,  5.22s/it]  6%|▌         | 32717/559320 [18:00<744:00:05,  5.09s/it]  6%|▌         | 32718/559320 [18:03<650:25:33,  4.45s/it]  6%|▌         | 32719/559320 [18:08<693:37:06,  4.74s/it]  6%|▌         | 32720/559320 [18:11<617:26:13,  4.22s/it]  6%|▌         | 32721/559320 [18:17<674:47:15,  4.61s/it]  6%|▌         | 32722/559320 [18:20<603:56:18,  4.13s/it]  6%|▌         | 32723/559320 [18:28<787:44:03,  5.39s/it]  6%|▌         | 32724/559320 [18:36<901:10:35,  6.16s/it]  6%|▌         | 32725/559320 [18:39<763:13:04,  5.22s/it]  6%|▌         | 32726/559320 [18:46<825:49:35,  5.65s/it]  6%|▌         | 32727/559320 [18:49<711:41:03,  4.87s/it]  6%|▌         | 32728/559320 [18:54<740:21:53,  5.06s/it]  6%|▌         | 32729/559320 [18:57<646:59:25,  4.42s/it]  6%|▌         | 32730/559320 [19:06<842:24:45,  5.76s/it]  6%|▌         | 32731/559320 [19:13<882:55:47,  6.04s/it]  6%|▌         | 32732/559320 [19:18<865:10:26,  5.91s/it]  6%|▌         | 32733/559320 [19:21<735:11:05,  5.03s/it]  6%|▌         | 32734/559320 [19:28<791:17:03,  5.41s/it]  6%|▌         | 32735/559320 [19:31<687:11:50,  4.70s/it]  6%|▌         | 32736/559320 [19:36<726:03:18,  4.96s/it]  6%|▌         | 32737/559320 [19:39<631:20:46,  4.32s/it]  6%|▌         | 32738/559320 [19:44<650:00:18,  4.44s/it]  6%|▌         | 32739/559320 [19:47<587:56:37,  4.02s/it]  6%|▌         | 32740/559320 [19:55<743:26:11,  5.08s/it]  6%|▌         | 32741/559320 [19:58<651:22:50,  4.45s/it]  6%|▌         | 32742/559320 [20:03<702:42:43,  4.80s/it]  6%|▌         | 32743/559320 [20:06<623:47:26,  4.26s/it]  6%|▌         | 32744/559320 [20:12<684:51:06,  4.68s/it]  6%|▌         | 32745/559320 [20:15<611:39:45,  4.18s/it]  6%|▌         | 32746/559320 [20:20<673:50:15,  4.61s/it]  6%|▌         | 32747/559320 [20:28<797:55:46,  5.46s/it]  6%|▌         | 32748/559320 [20:31<681:00:23,  4.66s/it]  6%|▌         | 32749/559320 [20:36<730:57:26,  5.00s/it]  6%|▌         | 32750/559320 [20:39<646:04:33,  4.42s/it]  6%|▌         | 32751/559320 [20:45<698:39:37,  4.78s/it]  6%|▌         | 32752/559320 [20:48<620:40:47,  4.24s/it]  6%|▌         | 32753/559320 [20:53<640:40:55,  4.38s/it]  6%|▌         | 32754/559320 [20:56<572:50:30,  3.92s/it]  6%|▌         | 32755/559320 [21:05<791:29:40,  5.41s/it]  6%|▌         | 32756/559320 [21:07<684:13:15,  4.68s/it]  6%|▌         | 32757/559320 [21:15<813:29:54,  5.56s/it]  6%|▌         | 32758/559320 [21:18<692:21:32,  4.73s/it]  6%|▌         | 32759/559320 [21:23<688:46:19,  4.71s/it]  6%|▌         | 32760/559320 [21:26<613:36:05,  4.20s/it]  6%|▌         | 32761/559320 [21:31<669:29:18,  4.58s/it]  6%|▌         | 32762/559320 [21:34<600:17:26,  4.10s/it]  6%|▌         | 32763/559320 [21:44<849:15:05,  5.81s/it]  6%|▌         | 32764/559320 [21:47<725:36:48,  4.96s/it]  6%|▌         | 32765/559320 [21:55<849:32:56,  5.81s/it]  6%|▌         | 32766/559320 [22:00<833:30:00,  5.70s/it]  6%|▌         | 32767/559320 [22:03<714:44:23,  4.89s/it]  6%|▌         | 32768/559320 [22:08<705:50:48,  4.83s/it]  6%|▌         | 32769/559320 [22:11<616:39:04,  4.22s/it]  6%|▌         | 32770/559320 [22:20<837:48:14,  5.73s/it]  6%|▌         | 32771/559320 [22:23<717:41:55,  4.91s/it]  6%|▌         | 32772/559320 [22:31<847:53:19,  5.80s/it]  6%|▌         | 32773/559320 [22:37<887:10:24,  6.07s/it]  6%|▌         | 32774/559320 [22:40<744:51:01,  5.09s/it]  6%|▌         | 32775/559320 [22:45<753:11:20,  5.15s/it]  6%|▌         | 32776/559320 [22:48<659:07:24,  4.51s/it]  6%|▌         | 32777/559320 [22:57<832:24:46,  5.69s/it]  6%|▌         | 32778/559320 [23:00<715:54:38,  4.89s/it]  6%|▌         | 32779/559320 [23:08<841:22:59,  5.75s/it]  6%|▌         | 32780/559320 [23:11<723:09:24,  4.94s/it]  6%|▌         | 32781/559320 [23:17<797:58:53,  5.46s/it]  6%|▌         | 32782/559320 [23:20<692:20:48,  4.73s/it]  6%|▌         | 32783/559320 [23:26<738:20:32,  5.05s/it]  6%|▌         | 32784/559320 [23:29<649:42:10,  4.44s/it]  6%|▌         | 32785/559320 [23:37<815:03:57,  5.57s/it]  6%|▌         | 32786/559320 [23:40<703:03:48,  4.81s/it]  6%|▌         | 32787/559320 [23:48<803:26:07,  5.49s/it]  6%|▌         | 32788/559320 [23:51<695:10:46,  4.75s/it]  6%|▌         | 32789/559320 [23:57<774:34:35,  5.30s/it]  6%|▌         | 32790/559320 [24:00<675:47:08,  4.62s/it]  6%|▌         | 32791/559320 [24:05<686:22:24,  4.69s/it]  6%|▌         | 32792/559320 [24:08<612:04:45,  4.18s/it]  6%|▌         | 32793/559320 [24:16<761:16:47,  5.21s/it]  6%|▌         | 32794/559320 [24:19<665:32:05,  4.55s/it]  6%|▌         | 32795/559320 [24:26<795:29:34,  5.44s/it]  6%|▌         | 32796/559320 [24:34<911:07:52,  6.23s/it]  6%|▌         | 32797/559320 [24:37<762:59:54,  5.22s/it]  6%|▌         | 32798/559320 [24:42<743:08:30,  5.08s/it]  6%|▌         | 32799/559320 [24:45<653:08:19,  4.47s/it]  6%|▌         | 32800/559320 [24:55<881:08:13,  6.02s/it]                                                            6%|▌         | 32800/559320 [24:55<881:08:13,  6.02s/it]  6%|▌         | 32801/559320 [24:58<750:26:18,  5.13s/it]  6%|▌         | 32802/559320 [25:05<867:30:21,  5.93s/it]  6%|▌         | 32803/559320 [25:08<739:19:31,  5.06s/it]  6%|▌         | 32804/559320 [25:17<884:53:30,  6.05s/it]  6%|▌         | 32805/559320 [25:20<746:01:57,  5.10s/it]  6%|▌         | 32806/559320 [25:25<744:58:27,  5.09s/it]  6%|▌         | 32807/559320 [25:28<644:49:57,  4.41s/it]  6%|▌         | 32808/559320 [25:36<810:20:25,  5.54s/it]  6%|▌         | 32809/559320 [25:39<697:09:49,  4.77s/it]  6%|▌         | 32810/559320 [25:46<794:50:47,  5.43s/it]  6%|▌         | 32811/559320 [25:48<678:18:07,  4.64s/it]  6%|▌         | 32812/559320 [25:56<798:06:45,  5.46s/it]  6%|▌         | 32813/559320 [25:59<686:44:50,  4.70s/it]  6%|▌         | 32814/559320 [26:04<724:22:05,  4.95s/it]  6%|▌         | 32815/559320 [26:07<637:17:54,  4.36s/it]  6%|▌         | 32816/559320 [26:15<782:21:35,  5.35s/it]  6%|▌         | 32817/559320 [26:23<891:48:38,  6.10s/it]  6%|▌         | 32818/559320 [26:26<755:58:54,  5.17s/it]  6%|▌         | 32819/559320 [26:33<855:11:06,  5.85s/it]  6%|▌         | 32820/559320 [26:36<721:05:26,  4.93s/it]  6%|▌         | 32821/559320 [26:42<773:43:57,  5.29s/it]  6%|▌         | 32822/559320 [26:45<675:44:51,  4.62s/it]  6%|▌         | 32823/559320 [26:53<819:34:43,  5.60s/it]  6%|▌         | 32824/559320 [26:56<707:52:16,  4.84s/it]  6%|▌         | 32825/559320 [27:02<741:43:03,  5.07s/it]  6%|▌         | 32826/559320 [27:11<940:39:28,  6.43s/it]  6%|▌         | 32827/559320 [27:14<790:05:42,  5.40s/it]  6%|▌         | 32828/559320 [27:21<857:37:29,  5.86s/it]  6%|▌         | 32829/559320 [27:24<733:02:04,  5.01s/it]  6%|▌         | 32830/559320 [27:33<895:02:09,  6.12s/it]  6%|▌         | 32831/559320 [27:36<759:19:01,  5.19s/it]  6%|▌         | 32832/559320 [27:44<888:26:11,  6.07s/it]  6%|▌         | 32833/559320 [27:47<750:34:53,  5.13s/it]  6%|▌         | 32834/559320 [27:53<782:03:53,  5.35s/it]  6%|▌         | 32835/559320 [27:56<679:31:54,  4.65s/it]  6%|▌         | 32836/559320 [28:04<810:03:36,  5.54s/it]  6%|▌         | 32837/559320 [28:07<699:10:47,  4.78s/it]  6%|▌         | 32838/559320 [28:14<828:46:58,  5.67s/it]  6%|▌         | 32839/559320 [28:17<703:48:18,  4.81s/it]  6%|▌         | 32840/559320 [28:22<711:30:25,  4.87s/it]  6%|▌         | 32841/559320 [28:25<631:15:14,  4.32s/it]  6%|▌         | 32842/559320 [28:35<867:27:36,  5.93s/it]  6%|▌         | 32843/559320 [28:38<737:19:27,  5.04s/it]  6%|▌         | 32844/559320 [28:47<927:33:02,  6.34s/it]  6%|▌         | 32845/559320 [28:50<781:51:22,  5.35s/it]  6%|▌         | 32846/559320 [28:56<797:10:54,  5.45s/it]  6%|▌         | 32847/559320 [28:59<688:58:04,  4.71s/it]  6%|▌         | 32848/559320 [29:05<755:27:16,  5.17s/it]  6%|▌         | 32849/559320 [29:08<660:12:09,  4.51s/it]  6%|▌         | 32850/559320 [29:17<853:31:49,  5.84s/it]  6%|▌         | 32851/559320 [29:24<897:55:15,  6.14s/it]  6%|▌         | 32852/559320 [29:27<763:49:05,  5.22s/it]  6%|▌         | 32853/559320 [29:32<773:05:00,  5.29s/it]  6%|▌         | 32854/559320 [29:35<672:55:35,  4.60s/it]  6%|▌         | 32855/559320 [29:43<786:43:26,  5.38s/it]  6%|▌         | 32856/559320 [29:46<680:22:52,  4.65s/it]  6%|▌         | 32857/559320 [29:57<984:02:07,  6.73s/it]  6%|▌         | 32858/559320 [30:00<825:34:37,  5.65s/it]  6%|▌         | 32859/559320 [30:08<928:21:51,  6.35s/it]  6%|▌         | 32860/559320 [30:11<774:15:32,  5.29s/it]  6%|▌         | 32861/559320 [30:16<772:58:20,  5.29s/it]  6%|▌         | 32862/559320 [30:19<674:30:59,  4.61s/it]  6%|▌         | 32863/559320 [30:28<828:24:20,  5.66s/it]  6%|▌         | 32864/559320 [30:31<717:28:45,  4.91s/it]  6%|▌         | 32865/559320 [30:39<879:27:35,  6.01s/it]  6%|▌         | 32866/559320 [30:46<929:10:31,  6.35s/it]  6%|▌         | 32867/559320 [30:49<774:56:15,  5.30s/it]  6%|▌         | 32868/559320 [30:55<807:16:42,  5.52s/it]  6%|▌         | 32869/559320 [30:58<698:15:30,  4.77s/it]  6%|▌         | 32870/559320 [31:06<829:10:49,  5.67s/it]  6%|▌         | 32871/559320 [31:09<712:15:20,  4.87s/it]  6%|▌         | 32872/559320 [31:18<884:48:30,  6.05s/it]  6%|▌         | 32873/559320 [31:21<742:44:49,  5.08s/it]  6%|▌         | 32874/559320 [31:30<925:33:51,  6.33s/it]  6%|▌         | 32875/559320 [31:33<781:22:36,  5.34s/it]  6%|▌         | 32876/559320 [31:38<756:15:02,  5.17s/it]  6%|▌         | 32877/559320 [31:41<653:03:44,  4.47s/it]  6%|▌         | 32878/559320 [31:46<710:42:15,  4.86s/it]  6%|▌         | 32879/559320 [31:49<630:19:21,  4.31s/it]  6%|▌         | 32880/559320 [31:57<774:21:28,  5.30s/it]  6%|▌         | 32881/559320 [32:05<891:49:23,  6.10s/it]  6%|▌         | 32882/559320 [32:08<747:58:53,  5.12s/it]  6%|▌         | 32883/559320 [32:11<654:58:50,  4.48s/it]  6%|▌         | 32884/559320 [32:17<712:39:22,  4.87s/it]  6%|▌         | 32885/559320 [32:20<631:11:07,  4.32s/it]  6%|▌         | 32886/559320 [32:25<691:02:36,  4.73s/it]  6%|▌         | 32887/559320 [32:28<615:45:20,  4.21s/it]  6%|▌         | 32888/559320 [32:36<767:09:03,  5.25s/it]  6%|▌         | 32889/559320 [32:39<669:01:02,  4.58s/it]  6%|▌         | 32890/559320 [32:47<820:33:00,  5.61s/it]  6%|▌         | 32891/559320 [32:50<705:25:02,  4.82s/it]  6%|▌         | 32892/559320 [32:57<784:46:11,  5.37s/it]  6%|▌         | 32893/559320 [33:00<681:08:22,  4.66s/it]  6%|▌         | 32894/559320 [33:04<687:14:01,  4.70s/it]  6%|▌         | 32895/559320 [33:07<605:07:33,  4.14s/it]  6%|▌         | 32896/559320 [33:15<746:34:09,  5.11s/it]  6%|▌         | 32897/559320 [33:17<644:55:06,  4.41s/it]  6%|▌         | 32898/559320 [33:26<817:44:23,  5.59s/it]  6%|▌         | 32899/559320 [33:33<899:13:10,  6.15s/it]  6%|▌         | 32900/559320 [33:36<763:46:58,  5.22s/it]                                                            6%|▌         | 32900/559320 [33:36<763:46:58,  5.22s/it]  6%|▌         | 32901/559320 [33:41<756:24:47,  5.17s/it]  6%|▌         | 32902/559320 [33:44<650:59:30,  4.45s/it]  6%|▌         | 32903/559320 [33:52<801:02:08,  5.48s/it]  6%|▌         | 32904/559320 [33:55<692:56:30,  4.74s/it]  6%|▌         | 32905/559320 [34:03<815:39:48,  5.58s/it]  6%|▌         | 32906/559320 [34:06<704:13:37,  4.82s/it]  6%|▌         | 32907/559320 [34:13<837:46:05,  5.73s/it]  6%|▌         | 32908/559320 [34:16<718:02:57,  4.91s/it]  6%|▌         | 32909/559320 [34:22<734:05:53,  5.02s/it]  6%|▌         | 32910/559320 [34:25<644:29:12,  4.41s/it]  6%|▌         | 32911/559320 [34:34<851:10:33,  5.82s/it]  6%|▌         | 32912/559320 [34:37<728:11:55,  4.98s/it]  6%|▌         | 32913/559320 [34:42<748:25:30,  5.12s/it]  6%|▌         | 32914/559320 [34:48<794:24:11,  5.43s/it]  6%|▌         | 32915/559320 [34:55<848:31:48,  5.80s/it]  6%|▌         | 32916/559320 [34:58<726:23:39,  4.97s/it]  6%|▌         | 32917/559320 [35:04<753:22:44,  5.15s/it]  6%|▌         | 32918/559320 [35:07<658:34:17,  4.50s/it]  6%|▌         | 32919/559320 [35:14<803:42:13,  5.50s/it]  6%|▌         | 32920/559320 [35:18<695:47:39,  4.76s/it]  6%|▌         | 32921/559320 [35:23<707:21:30,  4.84s/it]  6%|▌         | 32922/559320 [35:25<617:39:51,  4.22s/it]  6%|▌         | 32923/559320 [35:33<770:25:47,  5.27s/it]  6%|▌         | 32924/559320 [35:39<794:20:33,  5.43s/it]  6%|▌         | 32925/559320 [35:45<827:52:47,  5.66s/it]  6%|▌         | 32926/559320 [35:48<712:57:35,  4.88s/it]  6%|▌         | 32927/559320 [35:57<887:01:26,  6.07s/it]  6%|▌         | 32928/559320 [36:00<750:12:43,  5.13s/it]  6%|▌         | 32929/559320 [36:06<782:31:42,  5.35s/it]  6%|▌         | 32930/559320 [36:09<681:39:01,  4.66s/it]  6%|▌         | 32931/559320 [36:17<821:56:59,  5.62s/it]  6%|▌         | 32932/559320 [36:19<699:25:02,  4.78s/it]  6%|▌         | 32933/559320 [36:27<823:07:45,  5.63s/it]  6%|▌         | 32934/559320 [36:30<708:16:32,  4.84s/it]  6%|▌         | 32935/559320 [36:38<835:46:03,  5.72s/it]  6%|▌         | 32936/559320 [36:41<720:52:28,  4.93s/it]  6%|▌         | 32937/559320 [36:46<718:33:13,  4.91s/it]  6%|▌         | 32938/559320 [36:48<594:06:04,  4.06s/it]  6%|▌         | 32939/559320 [36:56<780:33:45,  5.34s/it]  6%|▌         | 32940/559320 [36:59<679:36:49,  4.65s/it]  6%|▌         | 32941/559320 [37:05<723:17:09,  4.95s/it]  6%|▌         | 32942/559320 [37:08<637:26:17,  4.36s/it]  6%|▌         | 32943/559320 [37:18<878:00:09,  6.00s/it]  6%|▌         | 32944/559320 [37:21<746:35:59,  5.11s/it]  6%|▌         | 32945/559320 [37:26<737:56:14,  5.05s/it]  6%|▌         | 32946/559320 [37:28<639:49:58,  4.38s/it]  6%|▌         | 32947/559320 [37:38<844:53:27,  5.78s/it]  6%|▌         | 32948/559320 [37:41<723:09:37,  4.95s/it]  6%|▌         | 32949/559320 [37:48<812:09:20,  5.55s/it]  6%|▌         | 32950/559320 [37:50<692:36:09,  4.74s/it]  6%|▌         | 32951/559320 [37:59<867:09:09,  5.93s/it]  6%|▌         | 32952/559320 [38:02<739:06:51,  5.06s/it]  6%|▌         | 32953/559320 [38:07<729:30:53,  4.99s/it]  6%|▌         | 32954/559320 [38:10<625:09:48,  4.28s/it]  6%|▌         | 32955/559320 [38:19<863:00:52,  5.90s/it]  6%|▌         | 32956/559320 [38:22<737:58:41,  5.05s/it]  6%|▌         | 32957/559320 [38:28<769:29:31,  5.26s/it]  6%|▌         | 32958/559320 [38:31<661:11:18,  4.52s/it]  6%|▌         | 32959/559320 [38:39<825:04:37,  5.64s/it]  6%|▌         | 32960/559320 [38:42<709:10:15,  4.85s/it]  6%|▌         | 32961/559320 [38:48<735:10:51,  5.03s/it]  6%|▌         | 32962/559320 [38:53<769:58:46,  5.27s/it]  6%|▌         | 32963/559320 [39:00<846:15:11,  5.79s/it]  6%|▌         | 32964/559320 [39:05<813:34:53,  5.56s/it]  6%|▌         | 32965/559320 [39:10<789:07:47,  5.40s/it]  6%|▌         | 32966/559320 [39:17<854:55:58,  5.85s/it]  6%|▌         | 32967/559320 [39:20<732:37:09,  5.01s/it]  6%|▌         | 32968/559320 [39:29<895:27:57,  6.12s/it]  6%|▌         | 32969/559320 [39:32<736:40:59,  5.04s/it]  6%|▌         | 32970/559320 [39:37<745:39:24,  5.10s/it]  6%|▌         | 32971/559320 [39:43<792:15:42,  5.42s/it]  6%|▌         | 32972/559320 [39:49<805:43:21,  5.51s/it]  6%|▌         | 32973/559320 [39:52<690:12:30,  4.72s/it]  6%|▌         | 32974/559320 [39:58<779:26:47,  5.33s/it]  6%|▌         | 32975/559320 [40:06<901:26:02,  6.17s/it]  6%|▌         | 32976/559320 [40:10<765:01:41,  5.23s/it]  6%|▌         | 32977/559320 [40:15<754:24:38,  5.16s/it]  6%|▌         | 32978/559320 [40:20<772:48:59,  5.29s/it]  6%|▌         | 32979/559320 [40:26<778:45:29,  5.33s/it]  6%|▌         | 32980/559320 [40:29<676:50:34,  4.63s/it]  6%|▌         | 32981/559320 [40:38<868:52:38,  5.94s/it]  6%|▌         | 32982/559320 [40:41<739:54:14,  5.06s/it]  6%|▌         | 32983/559320 [40:50<951:03:26,  6.50s/it]  6%|▌         | 32984/559320 [40:53<791:54:33,  5.42s/it]  6%|▌         | 32985/559320 [40:57<699:20:00,  4.78s/it]  6%|▌         | 32986/559320 [41:06<893:14:07,  6.11s/it]  6%|▌         | 32987/559320 [41:09<779:37:01,  5.33s/it]  6%|▌         | 32988/559320 [41:16<833:18:50,  5.70s/it]  6%|▌         | 32989/559320 [41:24<944:29:29,  6.46s/it]  6%|▌         | 32990/559320 [41:27<801:18:29,  5.48s/it]  6%|▌         | 32991/559320 [41:39<1084:37:31,  7.42s/it]  6%|▌         | 32992/559320 [41:43<903:50:34,  6.18s/it]   6%|▌         | 32993/559320 [41:48<892:07:14,  6.10s/it]  6%|▌         | 32994/559320 [41:51<755:28:41,  5.17s/it]  6%|▌         | 32995/559320 [42:00<918:20:07,  6.28s/it]  6%|▌         | 32996/559320 [42:03<776:16:49,  5.31s/it]  6%|▌         | 32997/559320 [42:12<934:16:20,  6.39s/it]  6%|▌         | 32998/559320 [42:15<786:41:31,  5.38s/it]  6%|▌         | 32999/559320 [42:24<949:13:47,  6.49s/it]  6%|▌         | 33000/559320 [42:28<839:31:30,  5.74s/it]                                                            6%|▌         | 33000/559320 [42:28<839:31:30,  5.74s/it]  6%|▌         | 33001/559320 [42:35<862:32:57,  5.90s/it]  6%|▌         | 33002/559320 [42:38<755:56:38,  5.17s/it]  6%|▌         | 33003/559320 [42:50<1069:59:57,  7.32s/it]  6%|▌         | 33004/559320 [42:57<1031:24:58,  7.05s/it]  6%|▌         | 33005/559320 [43:00<846:40:43,  5.79s/it]   6%|▌         | 33006/559320 [43:07<920:14:20,  6.29s/it]  6%|▌         | 33007/559320 [43:10<768:12:04,  5.25s/it]  6%|▌         | 33008/559320 [43:16<796:55:23,  5.45s/it]  6%|▌         | 33009/559320 [43:19<692:33:10,  4.74s/it]  6%|▌         | 33010/559320 [43:28<866:37:56,  5.93s/it]  6%|▌         | 33011/559320 [43:31<730:04:34,  4.99s/it]  6%|▌         | 33012/559320 [43:36<756:05:26,  5.17s/it]  6%|▌         | 33013/559320 [43:39<663:00:40,  4.54s/it]  6%|▌         | 33014/559320 [43:47<795:16:37,  5.44s/it]  6%|▌         | 33015/559320 [43:56<951:42:24,  6.51s/it]  6%|▌         | 33016/559320 [43:59<799:38:30,  5.47s/it]  6%|▌         | 33017/559320 [44:07<932:17:09,  6.38s/it]  6%|▌         | 33018/559320 [44:10<787:11:18,  5.38s/it]  6%|▌         | 33019/559320 [44:15<773:14:09,  5.29s/it]  6%|▌         | 33020/559320 [44:18<665:28:28,  4.55s/it]  6%|▌         | 33021/559320 [44:26<808:56:28,  5.53s/it]  6%|▌         | 33022/559320 [44:29<689:51:45,  4.72s/it]  6%|▌         | 33023/559320 [44:38<878:25:02,  6.01s/it]  6%|▌         | 33024/559320 [44:41<749:28:54,  5.13s/it]  6%|▌         | 33025/559320 [44:49<869:11:55,  5.95s/it]  6%|▌         | 33026/559320 [44:52<736:40:11,  5.04s/it]  6%|▌         | 33027/559320 [44:59<841:06:52,  5.75s/it]  6%|▌         | 33028/559320 [45:02<726:46:15,  4.97s/it]  6%|▌         | 33029/559320 [45:10<838:13:28,  5.73s/it]  6%|▌         | 33030/559320 [45:13<712:11:25,  4.87s/it]  6%|▌         | 33031/559320 [45:20<824:30:50,  5.64s/it]  6%|▌         | 33032/559320 [45:23<712:23:51,  4.87s/it]  6%|▌         | 33033/559320 [45:28<709:43:00,  4.85s/it]  6%|▌         | 33034/559320 [45:30<590:33:55,  4.04s/it]  6%|▌         | 33035/559320 [45:41<902:43:50,  6.18s/it]  6%|▌         | 33036/559320 [45:44<766:07:48,  5.24s/it]  6%|▌         | 33037/559320 [45:53<919:35:50,  6.29s/it]  6%|▌         | 33038/559320 [45:56<781:17:21,  5.34s/it]  6%|▌         | 33039/559320 [46:05<914:37:37,  6.26s/it]  6%|▌         | 33040/559320 [46:08<774:20:31,  5.30s/it]  6%|▌         | 33041/559320 [46:13<765:10:24,  5.23s/it]  6%|▌         | 33042/559320 [46:16<670:28:35,  4.59s/it]  6%|▌         | 33043/559320 [46:25<893:33:45,  6.11s/it]  6%|▌         | 33044/559320 [46:33<968:19:29,  6.62s/it]  6%|▌         | 33045/559320 [46:36<814:59:51,  5.58s/it]  6%|▌         | 33046/559320 [46:40<704:52:47,  4.82s/it]  6%|▌         | 33047/559320 [46:48<853:34:24,  5.84s/it]  6%|▌         | 33048/559320 [46:51<729:54:03,  4.99s/it]  6%|▌         | 33049/559320 [46:57<765:19:24,  5.24s/it]  6%|▌         | 33050/559320 [47:00<668:50:27,  4.58s/it]  6%|▌         | 33051/559320 [47:09<862:47:52,  5.90s/it]  6%|▌         | 33052/559320 [47:12<740:59:33,  5.07s/it]  6%|▌         | 33053/559320 [47:20<881:10:36,  6.03s/it]  6%|▌         | 33054/559320 [47:22<723:21:09,  4.95s/it]  6%|▌         | 33055/559320 [47:30<829:46:59,  5.68s/it]  6%|▌         | 33056/559320 [47:33<704:12:59,  4.82s/it]  6%|▌         | 33057/559320 [47:37<706:41:50,  4.83s/it]  6%|▌         | 33058/559320 [47:40<627:30:54,  4.29s/it]  6%|▌         | 33059/559320 [47:51<906:35:16,  6.20s/it]  6%|▌         | 33060/559320 [47:54<766:54:14,  5.25s/it]  6%|▌         | 33061/559320 [48:00<810:03:27,  5.54s/it]  6%|▌         | 33062/559320 [48:03<702:03:11,  4.80s/it]  6%|▌         | 33063/559320 [48:15<996:45:52,  6.82s/it]  6%|▌         | 33064/559320 [48:18<830:50:10,  5.68s/it]  6%|▌         | 33065/559320 [48:29<1083:59:23,  7.42s/it]  6%|▌         | 33066/559320 [48:32<890:01:23,  6.09s/it]   6%|▌         | 33067/559320 [48:38<870:55:42,  5.96s/it]  6%|▌         | 33068/559320 [48:41<741:40:23,  5.07s/it]  6%|▌         | 33069/559320 [48:46<734:53:53,  5.03s/it]  6%|▌         | 33070/559320 [48:49<646:42:21,  4.42s/it]  6%|▌         | 33071/559320 [49:00<923:08:23,  6.32s/it]  6%|▌         | 33072/559320 [49:03<781:20:07,  5.35s/it]  6%|▌         | 33073/559320 [49:08<756:53:35,  5.18s/it]  6%|▌         | 33074/559320 [49:10<653:10:42,  4.47s/it]  6%|▌         | 33075/559320 [49:20<854:29:57,  5.85s/it]  6%|▌         | 33076/559320 [49:23<733:12:58,  5.02s/it]  6%|▌         | 33077/559320 [49:28<754:03:04,  5.16s/it]  6%|▌         | 33078/559320 [49:31<662:35:09,  4.53s/it]  6%|▌         | 33079/559320 [49:39<799:25:46,  5.47s/it]  6%|▌         | 33080/559320 [49:42<691:51:40,  4.73s/it]  6%|▌         | 33081/559320 [49:47<731:23:56,  5.00s/it]  6%|▌         | 33082/559320 [49:51<645:25:40,  4.42s/it]  6%|▌         | 33083/559320 [49:58<795:12:29,  5.44s/it]  6%|▌         | 33084/559320 [50:01<690:30:16,  4.72s/it]  6%|▌         | 33085/559320 [50:06<691:44:27,  4.73s/it]  6%|▌         | 33086/559320 [50:09<614:54:28,  4.21s/it]  6%|▌         | 33087/559320 [50:19<843:10:42,  5.77s/it]  6%|▌         | 33088/559320 [50:22<724:16:23,  4.95s/it]  6%|▌         | 33089/559320 [50:27<741:50:49,  5.08s/it]  6%|▌         | 33090/559320 [50:30<649:26:33,  4.44s/it]  6%|▌         | 33091/559320 [50:41<939:18:00,  6.43s/it]  6%|▌         | 33092/559320 [50:44<791:41:28,  5.42s/it]  6%|▌         | 33093/559320 [50:50<818:28:09,  5.60s/it]  6%|▌         | 33094/559320 [51:00<988:10:36,  6.76s/it]  6%|▌         | 33095/559320 [51:03<835:25:22,  5.72s/it]  6%|▌         | 33096/559320 [51:12<1000:43:44,  6.85s/it]  6%|▌         | 33097/559320 [51:15<836:04:27,  5.72s/it]   6%|▌         | 33098/559320 [51:24<962:09:15,  6.58s/it]  6%|▌         | 33099/559320 [51:27<807:30:44,  5.52s/it]  6%|▌         | 33100/559320 [51:32<803:51:21,  5.50s/it]                                                            6%|▌         | 33100/559320 [51:33<803:51:21,  5.50s/it]  6%|▌         | 33101/559320 [51:36<700:01:30,  4.79s/it]  6%|▌         | 33102/559320 [51:45<884:36:16,  6.05s/it]  6%|▌         | 33103/559320 [51:50<867:47:25,  5.94s/it]  6%|▌         | 33104/559320 [51:58<927:19:44,  6.34s/it]  6%|▌         | 33105/559320 [52:01<783:14:38,  5.36s/it]  6%|▌         | 33106/559320 [52:06<795:42:34,  5.44s/it]  6%|▌         | 33107/559320 [52:09<695:28:40,  4.76s/it]  6%|▌         | 33108/559320 [52:20<951:45:07,  6.51s/it]  6%|▌         | 33109/559320 [52:23<798:41:46,  5.46s/it]  6%|▌         | 33110/559320 [52:29<829:33:38,  5.68s/it]  6%|▌         | 33111/559320 [52:32<716:10:19,  4.90s/it]  6%|▌         | 33112/559320 [52:43<961:38:08,  6.58s/it]  6%|▌         | 33113/559320 [52:46<808:46:28,  5.53s/it]  6%|▌         | 33114/559320 [52:52<814:52:19,  5.57s/it]  6%|▌         | 33115/559320 [52:55<705:35:43,  4.83s/it]  6%|▌         | 33116/559320 [53:03<871:31:13,  5.96s/it]  6%|▌         | 33117/559320 [53:06<733:59:44,  5.02s/it]  6%|▌         | 33118/559320 [53:14<844:46:06,  5.78s/it]  6%|▌         | 33119/559320 [53:17<727:57:01,  4.98s/it]  6%|▌         | 33120/559320 [53:26<928:59:13,  6.36s/it]  6%|▌         | 33121/559320 [53:29<784:04:16,  5.36s/it]  6%|▌         | 33122/559320 [53:39<953:10:44,  6.52s/it]  6%|▌         | 33123/559320 [53:42<800:49:57,  5.48s/it]  6%|▌         | 33124/559320 [53:47<780:36:22,  5.34s/it]  6%|▌         | 33125/559320 [53:50<678:44:26,  4.64s/it]  6%|▌         | 33126/559320 [53:58<835:12:38,  5.71s/it]  6%|▌         | 33127/559320 [54:01<721:09:34,  4.93s/it]  6%|▌         | 33128/559320 [54:09<845:49:57,  5.79s/it]  6%|▌         | 33129/559320 [54:12<717:28:59,  4.91s/it]  6%|▌         | 33130/559320 [54:17<752:16:37,  5.15s/it]  6%|▌         | 33131/559320 [54:20<644:53:25,  4.41s/it]  6%|▌         | 33132/559320 [54:29<826:06:03,  5.65s/it]  6%|▌         | 33133/559320 [54:32<712:23:03,  4.87s/it]  6%|▌         | 33134/559320 [54:40<851:20:24,  5.82s/it]  6%|▌         | 33135/559320 [54:43<732:19:36,  5.01s/it]  6%|▌         | 33136/559320 [54:51<876:19:55,  6.00s/it]  6%|▌         | 33137/559320 [54:54<749:06:24,  5.13s/it]  6%|▌         | 33138/559320 [55:04<934:09:26,  6.39s/it]  6%|▌         | 33139/559320 [55:07<786:50:13,  5.38s/it]  6%|▌         | 33140/559320 [55:11<765:54:13,  5.24s/it]  6%|▌         | 33141/559320 [55:14<639:50:33,  4.38s/it]  6%|▌         | 33142/559320 [55:24<895:26:38,  6.13s/it]  6%|▌         | 33143/559320 [55:27<757:42:41,  5.18s/it]  6%|▌         | 33144/559320 [55:34<821:46:24,  5.62s/it]  6%|▌         | 33145/559320 [55:37<710:02:59,  4.86s/it]  6%|▌         | 33146/559320 [55:47<940:30:42,  6.43s/it]  6%|▌         | 33147/559320 [55:50<790:29:28,  5.41s/it]  6%|▌         | 33148/559320 [55:55<789:59:49,  5.41s/it]  6%|▌         | 33149/559320 [55:58<685:06:22,  4.69s/it]  6%|▌         | 33150/559320 [56:08<883:49:08,  6.05s/it]  6%|▌         | 33151/559320 [56:11<756:10:41,  5.17s/it]  6%|▌         | 33152/559320 [56:17<829:04:27,  5.67s/it]  6%|▌         | 33153/559320 [56:20<712:18:20,  4.87s/it]  6%|▌         | 33154/559320 [56:28<843:16:27,  5.77s/it]  6%|▌         | 33155/559320 [56:31<715:05:58,  4.89s/it]  6%|▌         | 33156/559320 [56:37<751:51:38,  5.14s/it]  6%|▌         | 33157/559320 [56:40<660:52:01,  4.52s/it]  6%|▌         | 33158/559320 [56:48<819:18:31,  5.61s/it]  6%|▌         | 33159/559320 [56:51<703:36:42,  4.81s/it]  6%|▌         | 33160/559320 [57:00<884:16:36,  6.05s/it]  6%|▌         | 33161/559320 [57:03<755:43:07,  5.17s/it]  6%|▌         | 33162/559320 [57:14<1000:31:19,  6.85s/it]  6%|▌         | 33163/559320 [57:16<812:54:21,  5.56s/it]   6%|▌         | 33164/559320 [57:22<794:31:49,  5.44s/it]  6%|▌         | 33165/559320 [57:25<683:52:15,  4.68s/it]  6%|▌         | 33166/559320 [57:32<800:11:07,  5.47s/it]  6%|▌         | 33167/559320 [57:35<683:48:00,  4.68s/it]  6%|▌         | 33168/559320 [57:43<831:51:07,  5.69s/it]  6%|▌         | 33169/559320 [57:46<709:01:54,  4.85s/it]  6%|▌         | 33170/559320 [57:58<1028:11:10,  7.04s/it]  6%|▌         | 33171/559320 [58:01<852:38:32,  5.83s/it]   6%|▌         | 33172/559320 [58:06<809:16:11,  5.54s/it]  6%|▌         | 33173/559320 [58:08<690:53:14,  4.73s/it]  6%|▌         | 33174/559320 [58:16<817:55:27,  5.60s/it]  6%|▌         | 33175/559320 [58:19<710:20:40,  4.86s/it]  6%|▌         | 33176/559320 [58:26<800:48:52,  5.48s/it]  6%|▌         | 33177/559320 [58:29<694:46:43,  4.75s/it]  6%|▌         | 33178/559320 [58:38<875:12:36,  5.99s/it]  6%|▌         | 33179/559320 [58:41<737:49:28,  5.05s/it]  6%|▌         | 33180/559320 [58:46<733:48:27,  5.02s/it]  6%|▌         | 33181/559320 [58:53<837:19:23,  5.73s/it]  6%|▌         | 33182/559320 [58:56<716:52:48,  4.91s/it]  6%|▌         | 33183/559320 [59:05<877:12:05,  6.00s/it]  6%|▌         | 33184/559320 [59:08<746:24:42,  5.11s/it]  6%|▌         | 33185/559320 [59:11<652:26:56,  4.46s/it]  6%|▌         | 33186/559320 [59:20<874:36:22,  5.98s/it]  6%|▌         | 33187/559320 [59:23<734:38:16,  5.03s/it]  6%|▌         | 33188/559320 [59:29<771:57:13,  5.28s/it]  6%|▌         | 33189/559320 [59:32<666:20:17,  4.56s/it]  6%|▌         | 33190/559320 [59:42<909:51:47,  6.23s/it]  6%|▌         | 33191/559320 [59:45<772:20:05,  5.28s/it]  6%|▌         | 33192/559320 [59:50<758:15:56,  5.19s/it]  6%|▌         | 33193/559320 [59:53<660:37:52,  4.52s/it]  6%|▌         | 33194/559320 [1:00:04<940:31:42,  6.44s/it]  6%|▌         | 33195/559320 [1:00:10<915:19:51,  6.26s/it]  6%|▌         | 33196/559320 [1:00:13<774:54:02,  5.30s/it]  6%|▌         | 33197/559320 [1:00:21<887:53:34,  6.08s/it]  6%|▌         | 33198/559320 [1:00:24<754:39:49,  5.16s/it]  6%|▌         | 33199/559320 [1:00:29<746:36:36,  5.11s/it]  6%|▌         | 33200/559320 [1:00:32<645:11:24,  4.41s/it]                                                              6%|▌         | 33200/559320 [1:00:32<645:11:24,  4.41s/it]  6%|▌         | 33201/559320 [1:00:40<839:53:48,  5.75s/it]  6%|▌         | 33202/559320 [1:00:43<724:07:47,  4.95s/it]  6%|▌         | 33203/559320 [1:00:51<843:55:52,  5.77s/it]  6%|▌         | 33204/559320 [1:00:54<715:49:38,  4.90s/it]  6%|▌         | 33205/559320 [1:00:59<705:50:45,  4.83s/it]  6%|▌         | 33206/559320 [1:01:02<627:37:38,  4.29s/it]  6%|▌         | 33207/559320 [1:01:09<759:41:14,  5.20s/it]  6%|▌         | 33208/559320 [1:01:11<636:41:21,  4.36s/it]  6%|▌         | 33209/559320 [1:01:22<895:24:48,  6.13s/it]  6%|▌         | 33210/559320 [1:01:25<759:06:32,  5.19s/it]  6%|▌         | 33211/559320 [1:01:30<771:06:04,  5.28s/it]  6%|▌         | 33212/559320 [1:01:33<672:22:50,  4.60s/it]  6%|▌         | 33213/559320 [1:01:42<863:52:10,  5.91s/it]  6%|▌         | 33214/559320 [1:01:45<737:48:48,  5.05s/it]  6%|▌         | 33215/559320 [1:01:51<774:28:48,  5.30s/it]  6%|▌         | 33216/559320 [1:01:54<675:30:03,  4.62s/it]  6%|▌         | 33217/559320 [1:02:01<781:03:36,  5.34s/it]  6%|▌         | 33218/559320 [1:02:04<671:11:42,  4.59s/it]  6%|▌         | 33219/559320 [1:02:13<879:05:42,  6.02s/it]  6%|▌         | 33220/559320 [1:02:16<748:22:54,  5.12s/it]  6%|▌         | 33221/559320 [1:02:25<883:08:53,  6.04s/it]  6%|▌         | 33222/559320 [1:02:28<750:59:41,  5.14s/it]  6%|▌         | 33223/559320 [1:02:32<734:29:20,  5.03s/it]  6%|▌         | 33224/559320 [1:02:35<643:48:59,  4.41s/it]  6%|▌         | 33225/559320 [1:02:43<787:24:29,  5.39s/it]  6%|▌         | 33226/559320 [1:02:51<921:51:28,  6.31s/it]  6%|▌         | 33227/559320 [1:02:54<763:03:46,  5.22s/it]  6%|▌         | 33228/559320 [1:03:02<870:18:41,  5.96s/it]  6%|▌         | 33229/559320 [1:03:05<739:29:41,  5.06s/it]  6%|▌         | 33230/559320 [1:03:10<736:05:57,  5.04s/it]  6%|▌         | 33231/559320 [1:03:13<638:51:57,  4.37s/it]  6%|▌         | 33232/559320 [1:03:20<792:07:43,  5.42s/it]  6%|▌         | 33233/559320 [1:03:23<679:07:16,  4.65s/it]  6%|▌         | 33234/559320 [1:03:33<878:34:59,  6.01s/it]  6%|▌         | 33235/559320 [1:03:36<747:41:01,  5.12s/it]  6%|▌         | 33236/559320 [1:03:43<863:02:18,  5.91s/it]  6%|▌         | 33237/559320 [1:03:46<737:21:40,  5.05s/it]  6%|▌         | 33238/559320 [1:03:52<752:11:31,  5.15s/it]  6%|▌         | 33239/559320 [1:03:55<654:19:21,  4.48s/it]  6%|▌         | 33240/559320 [1:04:03<805:56:39,  5.52s/it]  6%|▌         | 33241/559320 [1:04:06<696:24:45,  4.77s/it]  6%|▌         | 33242/559320 [1:04:16<931:22:25,  6.37s/it]  6%|▌         | 33243/559320 [1:04:19<786:46:57,  5.38s/it]  6%|▌         | 33244/559320 [1:04:26<864:51:48,  5.92s/it]  6%|▌         | 33245/559320 [1:04:29<739:33:12,  5.06s/it]  6%|▌         | 33246/559320 [1:04:39<938:28:52,  6.42s/it]  6%|▌         | 33247/559320 [1:04:45<947:04:03,  6.48s/it]  6%|▌         | 33248/559320 [1:04:53<1002:33:00,  6.86s/it]  6%|▌         | 33249/559320 [1:04:56<824:58:31,  5.65s/it]   6%|▌         | 33250/559320 [1:05:06<1037:37:02,  7.10s/it]  6%|▌         | 33251/559320 [1:05:09<863:06:35,  5.91s/it]   6%|▌         | 33252/559320 [1:05:16<910:23:35,  6.23s/it]  6%|▌         | 33253/559320 [1:05:19<772:20:26,  5.29s/it]  6%|▌         | 33254/559320 [1:05:24<756:01:18,  5.17s/it]  6%|▌         | 33255/559320 [1:05:29<727:18:19,  4.98s/it]  6%|▌         | 33256/559320 [1:05:38<895:47:08,  6.13s/it]  6%|▌         | 33257/559320 [1:05:41<758:53:47,  5.19s/it]  6%|▌         | 33258/559320 [1:05:48<871:44:33,  5.97s/it]  6%|▌         | 33259/559320 [1:05:52<743:08:54,  5.09s/it]  6%|▌         | 33260/559320 [1:05:56<734:26:04,  5.03s/it]  6%|▌         | 33261/559320 [1:05:59<647:18:40,  4.43s/it]  6%|▌         | 33262/559320 [1:06:05<713:51:46,  4.89s/it]  6%|▌         | 33263/559320 [1:06:08<633:35:24,  4.34s/it]  6%|▌         | 33264/559320 [1:06:16<767:25:57,  5.25s/it]  6%|▌         | 33265/559320 [1:06:19<670:46:35,  4.59s/it]  6%|▌         | 33266/559320 [1:06:29<932:45:58,  6.38s/it]  6%|▌         | 33267/559320 [1:06:32<782:09:40,  5.35s/it]  6%|▌         | 33268/559320 [1:06:41<940:55:01,  6.44s/it]  6%|▌         | 33269/559320 [1:06:44<792:14:08,  5.42s/it]  6%|▌         | 33270/559320 [1:06:49<769:54:58,  5.27s/it]  6%|▌         | 33271/559320 [1:06:52<662:23:25,  4.53s/it]  6%|▌         | 33272/559320 [1:07:00<809:22:49,  5.54s/it]  6%|▌         | 33273/559320 [1:07:03<703:10:57,  4.81s/it]  6%|▌         | 33274/559320 [1:07:14<970:12:09,  6.64s/it]  6%|▌         | 33275/559320 [1:07:17<812:48:55,  5.56s/it]  6%|▌         | 33276/559320 [1:07:23<847:02:49,  5.80s/it]  6%|▌         | 33277/559320 [1:07:27<728:13:50,  4.98s/it]  6%|▌         | 33278/559320 [1:07:31<704:11:54,  4.82s/it]  6%|▌         | 33279/559320 [1:07:34<616:29:33,  4.22s/it]  6%|▌         | 33280/559320 [1:07:41<743:48:13,  5.09s/it]  6%|▌         | 33281/559320 [1:07:44<655:03:29,  4.48s/it]  6%|▌         | 33282/559320 [1:07:52<827:23:35,  5.66s/it]  6%|▌         | 33283/559320 [1:07:55<710:50:32,  4.86s/it]  6%|▌         | 33284/559320 [1:08:02<789:02:48,  5.40s/it]  6%|▌         | 33285/559320 [1:08:05<679:48:11,  4.65s/it]  6%|▌         | 33286/559320 [1:08:11<723:28:29,  4.95s/it]  6%|▌         | 33287/559320 [1:08:14<639:54:15,  4.38s/it]  6%|▌         | 33288/559320 [1:08:22<806:09:23,  5.52s/it]  6%|▌         | 33289/559320 [1:08:25<697:05:21,  4.77s/it]  6%|▌         | 33290/559320 [1:08:35<933:28:01,  6.39s/it]  6%|▌         | 33291/559320 [1:08:38<783:40:29,  5.36s/it]  6%|▌         | 33292/559320 [1:08:46<886:23:08,  6.07s/it]  6%|▌         | 33293/559320 [1:08:49<753:28:50,  5.16s/it]  6%|▌         | 33294/559320 [1:08:53<710:57:57,  4.87s/it]  6%|▌         | 33295/559320 [1:08:56<622:56:56,  4.26s/it]  6%|▌         | 33296/559320 [1:09:03<762:03:26,  5.22s/it]  6%|▌         | 33297/559320 [1:09:06<670:28:56,  4.59s/it]  6%|▌         | 33298/559320 [1:09:17<922:00:03,  6.31s/it]  6%|▌         | 33299/559320 [1:09:20<777:03:03,  5.32s/it]  6%|▌         | 33300/559320 [1:09:28<928:31:04,  6.35s/it]                                                              6%|▌         | 33300/559320 [1:09:28<928:31:04,  6.35s/it]  6%|▌         | 33301/559320 [1:09:32<785:35:27,  5.38s/it]  6%|▌         | 33302/559320 [1:09:37<806:13:32,  5.52s/it]  6%|▌         | 33303/559320 [1:09:40<698:14:07,  4.78s/it]  6%|▌         | 33304/559320 [1:09:49<866:47:28,  5.93s/it]  6%|▌         | 33305/559320 [1:09:52<742:26:53,  5.08s/it]  6%|▌         | 33306/559320 [1:10:04<1028:17:00,  7.04s/it]  6%|▌         | 33307/559320 [1:10:07<853:37:46,  5.84s/it]   6%|▌         | 33308/559320 [1:10:12<812:09:59,  5.56s/it]  6%|▌         | 33309/559320 [1:10:14<684:40:43,  4.69s/it]  6%|▌         | 33310/559320 [1:10:24<889:27:54,  6.09s/it]  6%|▌         | 33311/559320 [1:10:27<756:23:38,  5.18s/it]  6%|▌         | 33312/559320 [1:10:34<830:42:53,  5.69s/it]  6%|▌         | 33313/559320 [1:10:42<954:02:38,  6.53s/it]  6%|▌         | 33314/559320 [1:10:45<796:06:06,  5.45s/it]  6%|▌         | 33315/559320 [1:10:56<1024:07:06,  7.01s/it]  6%|▌         | 33316/559320 [1:10:59<853:03:56,  5.84s/it]   6%|▌         | 33317/559320 [1:11:07<965:38:52,  6.61s/it]  6%|▌         | 33318/559320 [1:11:10<806:25:54,  5.52s/it]  6%|▌         | 33319/559320 [1:11:17<882:59:49,  6.04s/it]  6%|▌         | 33320/559320 [1:11:20<747:12:13,  5.11s/it]  6%|▌         | 33321/559320 [1:11:28<851:01:55,  5.82s/it]  6%|▌         | 33322/559320 [1:11:31<721:47:18,  4.94s/it]  6%|▌         | 33323/559320 [1:11:39<873:51:57,  5.98s/it]  6%|▌         | 33324/559320 [1:11:42<742:14:33,  5.08s/it]  6%|▌         | 33325/559320 [1:11:47<735:33:50,  5.03s/it]  6%|▌         | 33326/559320 [1:11:50<649:32:13,  4.45s/it]  6%|▌         | 33327/559320 [1:11:56<728:05:20,  4.98s/it]  6%|▌         | 33328/559320 [1:11:59<634:17:22,  4.34s/it]  6%|▌         | 33329/559320 [1:12:08<846:12:09,  5.79s/it]  6%|▌         | 33330/559320 [1:12:11<722:17:36,  4.94s/it]  6%|▌         | 33331/559320 [1:12:19<848:33:41,  5.81s/it]  6%|▌         | 33332/559320 [1:12:22<723:47:42,  4.95s/it]  6%|▌         | 33333/559320 [1:12:29<808:46:30,  5.54s/it]  6%|▌         | 33334/559320 [1:12:32<689:53:53,  4.72s/it]  6%|▌         | 33335/559320 [1:12:38<736:07:36,  5.04s/it]  6%|▌         | 33336/559320 [1:12:40<640:03:39,  4.38s/it]  6%|▌         | 33337/559320 [1:12:48<771:21:10,  5.28s/it]  6%|▌         | 33338/559320 [1:12:51<672:59:52,  4.61s/it]  6%|▌         | 33339/559320 [1:12:57<720:50:00,  4.93s/it]  6%|▌         | 33340/559320 [1:13:05<859:53:05,  5.89s/it]  6%|▌         | 33341/559320 [1:13:12<926:45:00,  6.34s/it]  6%|▌         | 33342/559320 [1:13:15<784:17:58,  5.37s/it]  6%|▌         | 33343/559320 [1:13:20<756:01:32,  5.17s/it]  6%|▌         | 33344/559320 [1:13:28<883:26:50,  6.05s/it]  6%|▌         | 33345/559320 [1:13:31<752:21:39,  5.15s/it]  6%|▌         | 33346/559320 [1:13:37<768:44:41,  5.26s/it]  6%|▌         | 33347/559320 [1:13:40<671:06:50,  4.59s/it]  6%|▌         | 33348/559320 [1:13:49<892:49:26,  6.11s/it]  6%|▌         | 33349/559320 [1:13:52<756:19:59,  5.18s/it]  6%|▌         | 33350/559320 [1:13:58<759:15:23,  5.20s/it]  6%|▌         | 33351/559320 [1:14:00<653:54:53,  4.48s/it]  6%|▌         | 33352/559320 [1:14:09<846:00:17,  5.79s/it]  6%|▌         | 33353/559320 [1:14:12<726:12:23,  4.97s/it]  6%|▌         | 33354/559320 [1:14:20<865:39:18,  5.93s/it]  6%|▌         | 33355/559320 [1:14:23<734:41:20,  5.03s/it]  6%|▌         | 33356/559320 [1:14:33<920:19:58,  6.30s/it]  6%|▌         | 33357/559320 [1:14:36<779:48:03,  5.34s/it]  6%|▌         | 33358/559320 [1:14:41<786:05:23,  5.38s/it]  6%|▌         | 33359/559320 [1:14:44<684:21:32,  4.68s/it]  6%|▌         | 33360/559320 [1:14:51<769:10:28,  5.26s/it]  6%|▌         | 33361/559320 [1:14:54<674:33:05,  4.62s/it]  6%|▌         | 33362/559320 [1:15:01<788:05:06,  5.39s/it]  6%|▌         | 33363/559320 [1:15:04<687:58:48,  4.71s/it]  6%|▌         | 33364/559320 [1:15:15<934:31:37,  6.40s/it]  6%|▌         | 33365/559320 [1:15:18<788:25:12,  5.40s/it]  6%|▌         | 33366/559320 [1:15:22<740:05:40,  5.07s/it]  6%|▌         | 33367/559320 [1:15:25<649:44:50,  4.45s/it]  6%|▌         | 33368/559320 [1:15:34<857:35:56,  5.87s/it]  6%|▌         | 33369/559320 [1:15:37<735:33:49,  5.03s/it]  6%|▌         | 33370/559320 [1:15:42<731:19:47,  5.01s/it]  6%|▌         | 33371/559320 [1:15:45<644:57:18,  4.41s/it]  6%|▌         | 33372/559320 [1:15:53<812:37:55,  5.56s/it]  6%|▌         | 33373/559320 [1:15:59<793:55:07,  5.43s/it]  6%|▌         | 33374/559320 [1:16:02<689:42:25,  4.72s/it]  6%|▌         | 33375/559320 [1:16:09<792:53:44,  5.43s/it]  6%|▌         | 33376/559320 [1:16:12<680:56:46,  4.66s/it]  6%|▌         | 33377/559320 [1:16:20<837:33:40,  5.73s/it]  6%|▌         | 33378/559320 [1:16:23<720:49:16,  4.93s/it]  6%|▌         | 33379/559320 [1:16:33<950:43:27,  6.51s/it]  6%|▌         | 33380/559320 [1:16:36<796:37:47,  5.45s/it]  6%|▌         | 33381/559320 [1:16:43<880:20:00,  6.03s/it]  6%|▌         | 33382/559320 [1:16:46<751:07:15,  5.14s/it]  6%|▌         | 33383/559320 [1:16:59<1080:53:55,  7.40s/it]  6%|▌         | 33384/559320 [1:17:02<892:30:00,  6.11s/it]   6%|▌         | 33385/559320 [1:17:10<980:26:43,  6.71s/it]  6%|▌         | 33386/559320 [1:17:13<808:41:37,  5.54s/it]  6%|▌         | 33387/559320 [1:17:20<880:32:46,  6.03s/it]  6%|▌         | 33388/559320 [1:17:23<748:22:32,  5.12s/it]  6%|▌         | 33389/559320 [1:17:32<892:52:17,  6.11s/it]  6%|▌         | 33390/559320 [1:17:35<761:50:19,  5.21s/it]  6%|▌         | 33391/559320 [1:17:41<808:53:15,  5.54s/it]  6%|▌         | 33392/559320 [1:17:44<699:29:12,  4.79s/it]  6%|▌         | 33393/559320 [1:17:55<942:44:47,  6.45s/it]  6%|▌         | 33394/559320 [1:17:58<794:47:01,  5.44s/it]  6%|▌         | 33395/559320 [1:18:05<894:33:57,  6.12s/it]  6%|▌         | 33396/559320 [1:18:08<750:44:07,  5.14s/it]  6%|▌         | 33397/559320 [1:18:15<844:51:05,  5.78s/it]  6%|▌         | 33398/559320 [1:18:21<843:11:51,  5.77s/it]  6%|▌         | 33399/559320 [1:18:27<823:23:04,  5.64s/it]  6%|▌         | 33400/559320 [1:18:30<708:27:42,  4.85s/it]                                                              6%|▌         | 33400/559320 [1:18:30<708:27:42,  4.85s/it]  6%|▌         | 33401/559320 [1:18:38<875:59:27,  6.00s/it]  6%|▌         | 33402/559320 [1:18:41<745:06:53,  5.10s/it]  6%|▌         | 33403/559320 [1:18:50<926:47:49,  6.34s/it]  6%|▌         | 33404/559320 [1:18:54<783:15:57,  5.36s/it]  6%|▌         | 33405/559320 [1:19:01<870:02:01,  5.96s/it]  6%|▌         | 33406/559320 [1:19:04<743:12:44,  5.09s/it]  6%|▌         | 33407/559320 [1:19:10<787:16:40,  5.39s/it]  6%|▌         | 33408/559320 [1:19:13<683:46:40,  4.68s/it]  6%|▌         | 33409/559320 [1:19:21<830:19:38,  5.68s/it]  6%|▌         | 33410/559320 [1:19:28<867:37:30,  5.94s/it]  6%|▌         | 33411/559320 [1:19:36<993:18:53,  6.80s/it]  6%|▌         | 33412/559320 [1:19:39<818:27:20,  5.60s/it]  6%|▌         | 33413/559320 [1:19:45<824:42:19,  5.65s/it]  6%|▌         | 33414/559320 [1:19:48<700:19:12,  4.79s/it]  6%|▌         | 33415/559320 [1:19:57<883:25:41,  6.05s/it]  6%|▌         | 33416/559320 [1:20:00<752:01:10,  5.15s/it]  6%|▌         | 33417/559320 [1:20:07<822:04:39,  5.63s/it]  6%|▌         | 33418/559320 [1:20:10<709:25:08,  4.86s/it]  6%|▌         | 33419/559320 [1:20:18<844:24:20,  5.78s/it]  6%|▌         | 33420/559320 [1:20:20<713:15:24,  4.88s/it]  6%|▌         | 33421/559320 [1:20:28<812:03:36,  5.56s/it]  6%|▌         | 33422/559320 [1:20:31<702:15:52,  4.81s/it]  6%|▌         | 33423/559320 [1:20:37<777:29:58,  5.32s/it]  6%|▌         | 33424/559320 [1:20:40<679:23:22,  4.65s/it]  6%|▌         | 33425/559320 [1:20:46<732:18:06,  5.01s/it]  6%|▌         | 33426/559320 [1:20:49<647:06:30,  4.43s/it]  6%|▌         | 33427/559320 [1:21:00<913:14:59,  6.25s/it]  6%|▌         | 33428/559320 [1:21:03<771:52:51,  5.28s/it]  6%|▌         | 33429/559320 [1:21:11<892:10:26,  6.11s/it]  6%|▌         | 33430/559320 [1:21:14<760:13:37,  5.20s/it]  6%|▌         | 33431/559320 [1:21:23<919:12:42,  6.29s/it]  6%|▌         | 33432/559320 [1:21:26<773:00:57,  5.29s/it]  6%|▌         | 33433/559320 [1:21:33<862:29:42,  5.90s/it]  6%|▌         | 33434/559320 [1:21:43<1057:55:38,  7.24s/it]  6%|▌         | 33435/559320 [1:21:46<871:57:21,  5.97s/it]   6%|▌         | 33436/559320 [1:21:56<1028:30:42,  7.04s/it]  6%|▌         | 33437/559320 [1:21:59<841:56:44,  5.76s/it]   6%|▌         | 33438/559320 [1:22:06<932:54:13,  6.39s/it]  6%|▌         | 33439/559320 [1:22:09<785:59:52,  5.38s/it]  6%|▌         | 33440/559320 [1:22:18<927:16:35,  6.35s/it]  6%|▌         | 33441/559320 [1:22:21<783:58:21,  5.37s/it]  6%|▌         | 33442/559320 [1:22:31<983:15:40,  6.73s/it]  6%|▌         | 33443/559320 [1:22:34<820:59:40,  5.62s/it]  6%|▌         | 33444/559320 [1:22:42<939:28:58,  6.43s/it]  6%|▌         | 33445/559320 [1:22:45<784:42:01,  5.37s/it]  6%|▌         | 33446/559320 [1:22:52<853:09:03,  5.84s/it]  6%|▌         | 33447/559320 [1:22:55<732:37:38,  5.02s/it]  6%|▌         | 33448/559320 [1:23:01<743:19:05,  5.09s/it]  6%|▌         | 33449/559320 [1:23:04<654:31:49,  4.48s/it]  6%|▌         | 33450/559320 [1:23:12<823:37:27,  5.64s/it]  6%|▌         | 33451/559320 [1:23:18<833:30:27,  5.71s/it]  6%|▌         | 33452/559320 [1:23:26<923:35:18,  6.32s/it]  6%|▌         | 33453/559320 [1:23:29<779:26:11,  5.34s/it]  6%|▌         | 33454/559320 [1:23:36<860:51:53,  5.89s/it]  6%|▌         | 33455/559320 [1:23:41<847:26:33,  5.80s/it]  6%|▌         | 33456/559320 [1:23:46<802:10:10,  5.49s/it]  6%|▌         | 33457/559320 [1:23:49<685:58:03,  4.70s/it]  6%|▌         | 33458/559320 [1:23:57<829:20:32,  5.68s/it]  6%|▌         | 33459/559320 [1:24:04<902:39:19,  6.18s/it]  6%|▌         | 33460/559320 [1:24:07<765:55:16,  5.24s/it]  6%|▌         | 33461/559320 [1:24:14<819:01:53,  5.61s/it]  6%|▌         | 33462/559320 [1:24:17<705:29:53,  4.83s/it]  6%|▌         | 33463/559320 [1:24:24<803:18:58,  5.50s/it]  6%|▌         | 33464/559320 [1:24:27<696:34:09,  4.77s/it]  6%|▌         | 33465/559320 [1:24:35<853:16:51,  5.84s/it]  6%|▌         | 33466/559320 [1:24:38<731:38:00,  5.01s/it]  6%|▌         | 33467/559320 [1:24:46<857:21:44,  5.87s/it]  6%|▌         | 33468/559320 [1:24:49<734:46:29,  5.03s/it]  6%|▌         | 33469/559320 [1:24:58<889:02:49,  6.09s/it]  6%|▌         | 33470/559320 [1:25:01<755:37:30,  5.17s/it]  6%|▌         | 33471/559320 [1:25:07<795:52:17,  5.45s/it]  6%|▌         | 33472/559320 [1:25:10<690:52:40,  4.73s/it]  6%|▌         | 33473/559320 [1:25:15<716:43:05,  4.91s/it]  6%|▌         | 33474/559320 [1:25:24<884:04:47,  6.05s/it]  6%|▌         | 33475/559320 [1:25:27<746:38:53,  5.11s/it]  6%|▌         | 33476/559320 [1:25:34<843:01:03,  5.77s/it]  6%|▌         | 33477/559320 [1:25:37<715:21:06,  4.90s/it]  6%|▌         | 33478/559320 [1:25:42<731:43:02,  5.01s/it]  6%|▌         | 33479/559320 [1:25:46<648:44:42,  4.44s/it]  6%|▌         | 33480/559320 [1:25:53<759:29:22,  5.20s/it]  6%|▌         | 33481/559320 [1:25:56<663:51:56,  4.54s/it]  6%|▌         | 33482/559320 [1:26:04<826:38:00,  5.66s/it]  6%|▌         | 33483/559320 [1:26:07<712:27:31,  4.88s/it]  6%|▌         | 33484/559320 [1:26:10<630:33:52,  4.32s/it]  6%|▌         | 33485/559320 [1:26:17<764:52:21,  5.24s/it]  6%|▌         | 33486/559320 [1:26:20<667:54:22,  4.57s/it]  6%|▌         | 33487/559320 [1:26:28<787:35:15,  5.39s/it]  6%|▌         | 33488/559320 [1:26:31<688:15:06,  4.71s/it]  6%|▌         | 33489/559320 [1:26:39<857:57:40,  5.87s/it]  6%|▌         | 33490/559320 [1:26:42<723:50:38,  4.96s/it]  6%|▌         | 33491/559320 [1:26:50<856:52:55,  5.87s/it]  6%|▌         | 33492/559320 [1:26:58<949:45:03,  6.50s/it]  6%|▌         | 33493/559320 [1:27:01<791:38:51,  5.42s/it]  6%|▌         | 33494/559320 [1:27:08<844:23:50,  5.78s/it]  6%|▌         | 33495/559320 [1:27:11<721:12:42,  4.94s/it]  6%|▌         | 33496/559320 [1:27:20<929:21:37,  6.36s/it]  6%|▌         | 33497/559320 [1:27:22<740:42:43,  5.07s/it]  6%|▌         | 33498/559320 [1:27:32<922:55:38,  6.32s/it]  6%|▌         | 33499/559320 [1:27:35<778:40:54,  5.33s/it]  6%|▌         | 33500/559320 [1:27:42<862:59:25,  5.91s/it]                                                              6%|▌         | 33500/559320 [1:27:42<862:59:25,  5.91s/it]  6%|▌         | 33501/559320 [1:27:45<734:36:44,  5.03s/it]  6%|▌         | 33502/559320 [1:27:50<724:31:53,  4.96s/it]  6%|▌         | 33503/559320 [1:27:53<635:57:31,  4.35s/it]  6%|▌         | 33504/559320 [1:28:00<784:20:43,  5.37s/it]  6%|▌         | 33505/559320 [1:28:03<680:09:56,  4.66s/it]  6%|▌         | 33506/559320 [1:28:13<879:53:16,  6.02s/it]  6%|▌         | 33507/559320 [1:28:16<748:38:38,  5.13s/it]  6%|▌         | 33508/559320 [1:28:22<809:41:38,  5.54s/it]  6%|▌         | 33509/559320 [1:28:25<700:23:58,  4.80s/it]  6%|▌         | 33510/559320 [1:28:34<863:32:24,  5.91s/it]  6%|▌         | 33511/559320 [1:28:37<737:07:22,  5.05s/it]  6%|▌         | 33512/559320 [1:28:47<975:23:43,  6.68s/it]  6%|▌         | 33513/559320 [1:28:50<808:20:45,  5.53s/it]  6%|▌         | 33514/559320 [1:28:58<907:55:00,  6.22s/it]  6%|▌         | 33515/559320 [1:29:01<767:39:19,  5.26s/it]  6%|▌         | 33516/559320 [1:29:10<929:46:35,  6.37s/it]  6%|▌         | 33517/559320 [1:29:13<782:54:37,  5.36s/it]  6%|▌         | 33518/559320 [1:29:21<907:26:25,  6.21s/it]  6%|▌         | 33519/559320 [1:29:24<770:05:28,  5.27s/it]  6%|▌         | 33520/559320 [1:29:30<796:04:03,  5.45s/it]  6%|▌         | 33521/559320 [1:29:36<811:22:16,  5.56s/it]  6%|▌         | 33522/559320 [1:29:47<1056:36:49,  7.23s/it]  6%|▌         | 33523/559320 [1:29:50<873:43:23,  5.98s/it]   6%|▌         | 33524/559320 [1:29:59<988:02:54,  6.76s/it]  6%|▌         | 33525/559320 [1:30:02<824:01:23,  5.64s/it]  6%|▌         | 33526/559320 [1:30:09<901:12:04,  6.17s/it]  6%|▌         | 33527/559320 [1:30:12<763:35:38,  5.23s/it]  6%|▌         | 33528/559320 [1:30:23<1014:11:33,  6.94s/it]  6%|▌         | 33529/559320 [1:30:26<844:06:56,  5.78s/it]   6%|▌         | 33530/559320 [1:30:33<912:53:41,  6.25s/it]  6%|▌         | 33531/559320 [1:30:36<765:19:18,  5.24s/it]  6%|▌         | 33532/559320 [1:30:44<885:30:45,  6.06s/it]  6%|▌         | 33533/559320 [1:30:47<754:43:08,  5.17s/it]  6%|▌         | 33534/559320 [1:30:54<810:23:11,  5.55s/it]  6%|▌         | 33535/559320 [1:30:57<701:22:02,  4.80s/it]  6%|▌         | 33536/559320 [1:31:08<964:30:40,  6.60s/it]  6%|▌         | 33537/559320 [1:31:10<797:42:51,  5.46s/it]  6%|▌         | 33538/559320 [1:31:18<897:25:28,  6.14s/it]  6%|▌         | 33539/559320 [1:31:21<762:36:58,  5.22s/it]  6%|▌         | 33540/559320 [1:31:26<753:31:48,  5.16s/it]  6%|▌         | 33541/559320 [1:31:29<651:16:21,  4.46s/it]  6%|▌         | 33542/559320 [1:31:36<759:44:12,  5.20s/it]  6%|▌         | 33543/559320 [1:31:39<656:27:16,  4.49s/it]  6%|▌         | 33544/559320 [1:31:45<748:54:36,  5.13s/it]  6%|▌         | 33545/559320 [1:31:52<792:57:46,  5.43s/it]  6%|▌         | 33546/559320 [1:32:02<996:14:14,  6.82s/it]  6%|▌         | 33547/559320 [1:32:05<829:29:31,  5.68s/it]  6%|▌         | 33548/559320 [1:32:12<892:13:29,  6.11s/it]  6%|▌         | 33549/559320 [1:32:15<759:11:59,  5.20s/it]  6%|▌         | 33550/559320 [1:32:22<859:11:06,  5.88s/it]  6%|▌         | 33551/559320 [1:32:25<729:28:56,  4.99s/it]  6%|▌         | 33552/559320 [1:32:34<897:59:53,  6.15s/it]  6%|▌         | 33553/559320 [1:32:37<764:21:58,  5.23s/it]  6%|▌         | 33554/559320 [1:32:46<911:45:38,  6.24s/it]  6%|▌         | 33555/559320 [1:32:49<759:32:12,  5.20s/it]  6%|▌         | 33556/559320 [1:32:56<848:48:29,  5.81s/it]  6%|▌         | 33557/559320 [1:32:59<729:05:33,  4.99s/it]  6%|▌         | 33558/559320 [1:33:05<781:11:32,  5.35s/it]  6%|▌         | 33559/559320 [1:33:10<765:01:42,  5.24s/it]  6%|▌         | 33560/559320 [1:33:13<669:16:31,  4.58s/it]  6%|▌         | 33561/559320 [1:33:24<940:07:33,  6.44s/it]  6%|▌         | 33562/559320 [1:33:27<790:49:38,  5.41s/it]  6%|▌         | 33563/559320 [1:33:36<952:35:04,  6.52s/it]  6%|▌         | 33564/559320 [1:33:39<800:38:50,  5.48s/it]  6%|▌         | 33565/559320 [1:33:45<826:32:10,  5.66s/it]  6%|▌         | 33566/559320 [1:33:48<713:05:52,  4.88s/it]  6%|▌         | 33567/559320 [1:33:53<710:53:06,  4.87s/it]  6%|▌         | 33568/559320 [1:33:56<634:06:29,  4.34s/it]  6%|▌         | 33569/559320 [1:34:06<876:31:26,  6.00s/it]  6%|▌         | 33570/559320 [1:34:09<745:35:05,  5.11s/it]  6%|▌         | 33571/559320 [1:34:18<892:37:02,  6.11s/it]  6%|▌         | 33572/559320 [1:34:21<765:08:28,  5.24s/it]  6%|▌         | 33573/559320 [1:34:28<835:54:32,  5.72s/it]  6%|▌         | 33574/559320 [1:34:30<709:19:00,  4.86s/it]  6%|▌         | 33575/559320 [1:34:35<720:17:15,  4.93s/it]  6%|▌         | 33576/559320 [1:34:38<627:35:34,  4.30s/it]  6%|▌         | 33577/559320 [1:34:47<814:37:09,  5.58s/it]